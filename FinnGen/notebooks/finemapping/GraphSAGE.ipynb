{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00910a35-ffbc-4497-bcc8-21f008c326fb",
   "metadata": {},
   "source": [
    "# Grapher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd1805a-55dc-409c-9ccf-512eb1f88667",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a8ec1d-73be-4b8c-95dc-79489e84998b",
   "metadata": {},
   "source": [
    "- `id`: This column represents the id of the variant in the following format: #chrom:pos:ref:alt (string).\n",
    "\n",
    "- `#chrom`: This column represents the chromosome number where the genetic variant is located.\n",
    "\n",
    "- `pos`: This is the position of the genetic variant on the chromosome.\n",
    "\n",
    "- `ref`: This column represents the reference allele (or variant) at the genomic position.\n",
    "\n",
    "- `alt`: This is the alternate allele observed at this position.\n",
    "\n",
    "- `rsids`: This stands for reference SNP cluster ID. It's a unique identifier for each variant used in the dbSNP database.\n",
    "\n",
    "- `nearest_genes`: This column represents the gene which is nearest to the variant.\n",
    "\n",
    "- `pval`: This represents the p-value, which is a statistical measure for the strength of evidence against the null hypothesis.\n",
    "\n",
    "- `mlogp`: This represents the minus log of the p-value, commonly used in genomic studies.\n",
    "\n",
    "- `beta`: The beta coefficient represents the effect size of the variant.\n",
    "\n",
    "- `sebeta`: This is the standard error of the beta coefficient.\n",
    "\n",
    "- `af_alt`: This is the allele frequency of the alternate variant in the general population.\n",
    "\n",
    "- `af_alt_cases`: This is the allele frequency of the alternate variant in the cases group.\n",
    "\n",
    "- `af_alt_controls`: This is the allele frequency of the alternate variant in the control group.\n",
    "\n",
    "- `finemapped`: This column represents whether the variant is included in the post-finemapped dataset (1) or not (0). \n",
    "\n",
    "- `trait`: This column represents the trait associated with the variant. In this dataset, it is the response to the drug paracetamol and NSAIDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113b033d-95f5-4584-b10c-72969a166612",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4345459-b1ef-477c-8f06-c8f54c194af0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.4 (tags/v3.11.4:d2340ef, Jun  7 2023, 05:45:37) [MSC v.1934 64 bit (AMD64)]\n",
      "NumPy version: 1.24.1\n",
      "Pandas version: 1.5.3\n",
      "Matplotlib version: 3.7.1\n",
      "Scikit-learn version: 1.3.0\n",
      "Torch version: 2.0.1+cu117\n",
      "Torch Geometric version: 2.3.1\n",
      "NetworkX version: 3.0\n",
      "Using NVIDIA RTX A6000 (cuda)\n",
      "CUDA version: 11.7\n",
      "Number of CUDA devices: 2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from numba import jit, prange\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder, StandardScaler, OrdinalEncoder, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.utils import to_undirected, negative_sampling\n",
    "import networkx as nx\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.special import expit\n",
    "from typing import List, Dict\n",
    "import time\n",
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "import category_encoders as ce\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "import copy\n",
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "# Print versions of imported libraries\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Matplotlib version: {matplotlib.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Torch Geometric version: {torch_geometric.__version__}\")\n",
    "print(f\"NetworkX version: {nx.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # Current CUDA device\n",
    "    print(f\"Using {torch.cuda.get_device_name()} ({device})\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddfa8cb-01df-491f-94f0-c8c228256b4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05fe5b06-7392-4d62-a1e8-af081cdf3e6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'id': 'string',\n",
    "    '#chrom': 'int64',\n",
    "    'pos': 'int64',\n",
    "    'ref': 'string',\n",
    "    'alt': 'string',\n",
    "    'rsids': 'string',\n",
    "    'nearest_genes': 'string',\n",
    "    'pval': 'float64',\n",
    "    'mlogp': 'float64',\n",
    "    'beta': 'float64',\n",
    "    'sebeta': 'float64',\n",
    "    'af_alt': 'float64',\n",
    "    'af_alt_cases': 'float64',\n",
    "    'af_alt_controls': 'float64',\n",
    "    'finemapped': 'int64'\n",
    "}\n",
    "\n",
    "data = pd.read_csv('~/Desktop/gwas-graph/FinnGen/data/gwas-finemap.csv', dtype=dtypes)\n",
    "\n",
    "# Assert column names\n",
    "expected_columns = ['#chrom', 'pos', 'ref', 'alt', 'rsids', 'nearest_genes', 'pval', 'mlogp', 'beta',\n",
    "                    'sebeta', 'af_alt', 'af_alt_cases', 'af_alt_controls', 'finemapped',\n",
    "                    'id', 'trait']\n",
    "assert set(data.columns) == set(expected_columns), \"Unexpected columns in the data DataFrame.\"\n",
    "\n",
    "# Assert data types\n",
    "expected_dtypes = {\n",
    "    'id': 'string',\n",
    "    '#chrom': 'int64',\n",
    "    'pos': 'int64',\n",
    "    'ref': 'string',\n",
    "    'alt': 'string',\n",
    "    'rsids': 'string',\n",
    "    'nearest_genes': 'string',\n",
    "    'pval': 'float64',\n",
    "    'mlogp': 'float64',\n",
    "    'beta': 'float64',\n",
    "    'sebeta': 'float64',\n",
    "    'af_alt': 'float64',\n",
    "    'af_alt_cases': 'float64',\n",
    "    'af_alt_controls': 'float64',\n",
    "    'finemapped': 'int64'\n",
    "}\n",
    "\n",
    "for col, expected_dtype in expected_dtypes.items():\n",
    "    assert data[col].dtype == expected_dtype, f\"Unexpected data type for column {col}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602a754a-7836-439e-801c-7efca6d2dfc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "513cafa0-f3f8-47b5-9d22-96062449edde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.sample(frac=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460fab1f-46fa-40a5-a8c5-aa746b1db7eb",
   "metadata": {},
   "source": [
    "### Find nearest gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b729972-b0d7-4e38-b8e2-aedd07506c98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['nearest_genes'] = data['nearest_genes'].astype(str)\n",
    "\n",
    "# Assert column 'nearest_genes' is a string\n",
    "assert data['nearest_genes'].dtype == 'object', \"Column 'nearest_genes' is not of string type.\"\n",
    "\n",
    "# Get the length of the data before transformation\n",
    "original_length = len(data)\n",
    "\n",
    "# Extract the first gene name from the 'nearest_genes' column\n",
    "data['nearest_genes'] = data['nearest_genes'].str.split(',').str[0]\n",
    "\n",
    "# Reset index to have a standard index\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Assert the length of the data remains the same\n",
    "assert len(data) == original_length, \"Length of the data has changed after transformation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eddaed-2f48-4440-b6d6-0dc247f23ea8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe89142-2b65-469b-b330-1620d303312b",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "`data` Pandas DataFrame:\n",
    "\n",
    "- `id`: This column represents the id of the variant in the following format: #chrom:pos:ref:alt (string).\n",
    "- `#chrom`: This column represents the chromosome number where the genetic variant is located.\n",
    "- `pos`: This is the position of the genetic variant on the chromosome (int: 1-200,000).\n",
    "- `ref`: This column represents the reference allele (or variant) at the genomic position.\n",
    "- `alt`: This is the alternate allele observed at this position.\n",
    "- `rsids`: This stands for reference SNP cluster ID. It's a unique identifier for each variant used in the dbSNP database.\n",
    "- `nearest_genes`: This column represents the gene which is nearest to the variant (string).\n",
    "- `pval`: This represents the p-value, which is a statistical measure for the strength of evidence against the null hypothesis.\n",
    "- `mlogp`: This represents the minus log of the p-value, commonly used in genomic studies.\n",
    "- `beta`: The beta coefficient represents the effect size of the variant.\n",
    "- `sebeta`: This is the standard error of the beta coefficient.\n",
    "- `af_alt`: This is the allele frequency of the alternate variant in the general population (float: 0-1.\n",
    "- `af_alt_cases`: This is the allele frequency of the alternate variant in the cases group (float: 0-1).\n",
    "- `af_alt_controls`: This is the allele frequency of the alternate variant in the control group (float: 0-1).\n",
    "- `finemapped`: This column represents whether the variant is included in the post-finemapped dataset (1) or not (0) (int).\n",
    "- `trait`: This column represents the trait associated with the variant. In this dataset, it is the response to the drug paracetamol and NSAIDs.\n",
    "\n",
    "### Task Overview\n",
    "\n",
    "The objective is to design and implement a binary node classification GNN model to predict whether variants are included after post-finemapping or not based on `finemapping`.\n",
    "\n",
    "### Nodes and Their Features\n",
    "\n",
    "There is one type of node: SNP nodes.\n",
    "\n",
    "- **SNP Nodes**: Each SNP Node is characterized by various features, including `id`, `nearest_genes`, `#chrom`, `pos`, `ref`, `alt`, `mlogp`, `beta`, `sebeta`,  `af_alt`, `af_alt_cases`, and `af_alt_controls` columns.\n",
    "\n",
    "### Edges, Their Features, and Labels\n",
    "\n",
    "Edges represent relationships between SNP nodes in the graph.\n",
    "\n",
    "1. **Type 1 Edges: LD-based edges**\n",
    "\n",
    "   - For each pair of SNPs (row1 and row2) that exist on the same chromosome (`#chrom`), an edge is created if 1) both SNPs have `finemapped=1` or `finemapped=0`, and 2) the absolute difference between their positions (`pos`) is less than or equal to 1,000,000 and greater than 1 (no loops).\n",
    "   - The following formula determines the weight of the edge:\n",
    "     \n",
    "```\n",
    "    weights = 1 * e^(-ln(2) / 100_000 * pos_diff_abs)\n",
    "    \n",
    "```\n",
    "\n",
    "    - Standardize the edge weights for each chromosome after all weights have been computed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51485cfb-989a-430f-874b-af2d5f79d9f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Graph creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a1ef2ec-c8a3-48f1-8a6e-9823bae3e4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import category_encoders as ce\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cProfile, pstats, io\n",
    "import time\n",
    "from typing import Tuple\n",
    "import torch.multiprocessing as mp\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from category_encoders import CountEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68257919-e9e4-4aa7-8b45-d0de2dbd765b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 227674\n",
      "Number of edges: 374656810\n",
      "Node feature dimension: 12\n",
      "Execution time: 138.54895973205566 seconds\n",
      "         8508827 function calls (8507698 primitive calls) in 138.161 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 1198 to 5 due to restriction <5>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "       18    0.000    0.000  138.559    7.698 C:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3472(run_code)\n",
      "       18    0.000    0.000  138.559    7.698 {built-in method builtins.exec}\n",
      "        1    9.742    9.742  135.174  135.174 C:\\Users\\Windows\\AppData\\Local\\Temp\\ipykernel_27548\\2354801936.py:89(preprocess_positive_edges)\n",
      "       50   70.776    1.416   70.776    1.416 {built-in method numpy.array}\n",
      "        1    2.884    2.884   50.458   50.458 C:\\Users\\Windows\\AppData\\Local\\Temp\\ipykernel_27548\\2354801936.py:46(process_data)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize cuda device, replace \"cuda:0\" and \"cuda:1\" with the GPU ids you want to use\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def get_unique_snps(data: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Function to create mappings for SNPs to integer indices.\n",
    "    \"\"\"\n",
    "    return {snp: idx for idx, snp in enumerate(data['id'].unique())}\n",
    "\n",
    "\n",
    "def preprocess_snp_features(data: pd.DataFrame, snp_to_idx: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to create node feature vectors for SNPs and preprocess categorical and numerical features.\n",
    "    \"\"\"\n",
    "    # Ensure that 'id' exists in the data and 'id' and 'nearest_genes' are not null\n",
    "    assert 'id' in data.columns and 'nearest_genes' in data.columns, \"Columns 'id' or 'nearest_genes' do not exist in the dataframe\"\n",
    "    assert data[['id', 'nearest_genes']].isnull().sum().sum() == 0, \"Columns 'id' or 'nearest_genes' contain null values\"\n",
    "\n",
    "    # Columns to be extracted from the original dataframe\n",
    "    cols_to_extract = ['id', 'nearest_genes', '#chrom', 'pos', 'ref', 'alt', 'mlogp', 'beta', 'sebeta', 'af_alt',\n",
    "                       'af_alt_cases', 'af_alt_controls', 'finemapped']\n",
    "\n",
    "    snp_features = data.loc[data['id'].isin(snp_to_idx.keys()), cols_to_extract].set_index('id').sort_index()\n",
    "    scaler = RobustScaler()\n",
    "\n",
    "    # Frequency encoding for 'nearest_genes' using LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    snp_features['nearest_genes'] = label_encoder.fit_transform(snp_features['nearest_genes'])\n",
    "\n",
    "    categorical_cols = ['ref', 'alt']\n",
    "\n",
    "    # CountEncoder for 'ref', 'alt'\n",
    "    count_encoder = ce.CountEncoder(cols=categorical_cols)\n",
    "    snp_features = count_encoder.fit_transform(snp_features)\n",
    "\n",
    "    numerical_cols = list(set(snp_features.columns) - set(categorical_cols))\n",
    "    snp_features[numerical_cols] = scaler.fit_transform(snp_features[numerical_cols])\n",
    "\n",
    "    # Filling 0 values for all columns\n",
    "    snp_features = snp_features.fillna(0)\n",
    "\n",
    "    return snp_features\n",
    "\n",
    "\n",
    "def process_data(data, window_size, device):\n",
    "    log2 = torch.log(torch.tensor(2., device=device))  # Pre-compute log(2)\n",
    "    positive_edges_snp_snp = []\n",
    "    snp_weights = []\n",
    "\n",
    "    for chrom, group in data.groupby('#chrom'):\n",
    "        if group.empty:\n",
    "            continue\n",
    "\n",
    "        # Convert necessary columns to numpy arrays for faster processing\n",
    "        group_snp_idx = torch.tensor(group['snp_idx'].values, device=device)\n",
    "        group_pos = torch.tensor(group['pos'].values, device=device)\n",
    "        group_finemapped = torch.tensor(group['finemapped'].values, device=device)\n",
    "\n",
    "        # Slide window across the group with overlap\n",
    "        overlap = window_size // 2  # 50% overlap\n",
    "        for i in range(0, len(group) - overlap, overlap):\n",
    "            window_snp_idx = group_snp_idx[i:i + window_size]\n",
    "            window_pos = group_pos[i:i + window_size]\n",
    "            window_finemapped = group_finemapped[i:i + window_size]\n",
    "\n",
    "            # Skip if window is empty\n",
    "            if len(window_pos) == 0:\n",
    "                continue\n",
    "\n",
    "            # Calculate differences in position for each pair of SNPs in the window\n",
    "            pos_diff_abs = torch.abs(window_pos[None, :] - window_pos[:, None])  # vectorized subtraction\n",
    "            finemapped_same = window_finemapped[None, :] == window_finemapped[:, None]  # whether both SNPs have finemapped=1 or finemapped=0\n",
    "\n",
    "            mask = finemapped_same & (pos_diff_abs > 1) & (pos_diff_abs <= 300_000)\n",
    "\n",
    "            weights = torch.exp(-log2 / 100_000 * pos_diff_abs[mask])\n",
    "            if len(weights) == 0:\n",
    "                continue\n",
    "\n",
    "            indices = torch.nonzero(mask)\n",
    "            positive_edges_snp_snp.extend(\n",
    "                zip(window_snp_idx[indices[:, 0]].cpu().numpy(), window_snp_idx[indices[:, 1]].cpu().numpy()))\n",
    "            snp_weights.extend(weights.cpu().numpy())\n",
    "\n",
    "    return positive_edges_snp_snp, snp_weights\n",
    "\n",
    "\n",
    "def preprocess_positive_edges(data: pd.DataFrame, snp_to_idx: dict, window_size: int = 30_000) -> Tuple[\n",
    "    torch.Tensor, torch.Tensor]:\n",
    "    scaler = RobustScaler()\n",
    "\n",
    "    # Sort data once before grouping\n",
    "    data = data.sort_values(by=['#chrom', 'pos'])\n",
    "\n",
    "    # Create new column for SNP index\n",
    "    data['snp_idx'] = data['id'].map(snp_to_idx)\n",
    "\n",
    "    # Process the data\n",
    "    positive_edges_snp_snp, snp_weights = process_data(data, window_size, device)\n",
    "\n",
    "    # Normalize weights and shift the computations to GPU\n",
    "    if snp_weights:  # Added condition here\n",
    "        snp_weights = np.array(snp_weights, dtype=np.float32).reshape(-1, 1)\n",
    "        scaler = scaler.fit(snp_weights)\n",
    "        snp_weights = torch.from_numpy(scaler.transform(snp_weights)).to(device).flatten()\n",
    "    else:\n",
    "        snp_weights = torch.tensor([], dtype=torch.float).to(device)\n",
    "\n",
    "    # Now, keep the edges on the CPU\n",
    "    positive_edges_snp_snp = torch.from_numpy(np.array(positive_edges_snp_snp, dtype=np.int64)).T\n",
    "\n",
    "    return positive_edges_snp_snp, snp_weights\n",
    "\n",
    "\n",
    "def create_pytorch_graph(features: torch.Tensor, edges: torch.Tensor, edge_weights: torch.Tensor) -> Data:\n",
    "    return Data(x=features, edge_index=edges, edge_attr=edge_weights)\n",
    "\n",
    "\n",
    "# Profiling\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "start_time = time.time()\n",
    "\n",
    "# Predefined chromosome number\n",
    "chromosome = 7  # You can set your desired chromosome here\n",
    "\n",
    "# Main\n",
    "data = data[data['#chrom'] == chromosome]  # Only keep the data for the specified chromosome\n",
    "\n",
    "snp_to_idx = get_unique_snps(data)\n",
    "labels = data['finemapped'].map(lambda x: 1 if x > 0 else 0)\n",
    "snp_features = preprocess_snp_features(data, snp_to_idx)\n",
    "\n",
    "# Now, keep the features on the CPU\n",
    "features = torch.tensor(snp_features.values, dtype=torch.float)\n",
    "\n",
    "# Preprocess edges and weights\n",
    "edges, weights = preprocess_positive_edges(data, snp_to_idx)\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Create the final graph (on the CPU)\n",
    "graph = create_pytorch_graph(features, edges, weights)\n",
    "graph.y = torch.tensor(labels.values, dtype=torch.long)\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Number of nodes: {graph.num_nodes}\")\n",
    "print(f\"Number of edges: {graph.num_edges}\")\n",
    "print(f\"Node feature dimension: {graph.num_node_features}\")\n",
    "\n",
    "# Profiling\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Execution time: {elapsed_time} seconds\")\n",
    "pr.disable()\n",
    "s = io.StringIO()\n",
    "sortby = 'cumulative'\n",
    "ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "ps.print_stats(5)  # Only print the top 5 lines\n",
    "print(s.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd4462c-4130-4242-9a7d-09c6ed71b613",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Graph stats"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7fe477a0-d560-425a-9537-42c525a33d71",
   "metadata": {
    "tags": []
   },
   "source": [
    "from torch_geometric.utils import degree\n",
    "\n",
    "def print_graph_stats(graph, positive_edges_snp_snp, features_list):\n",
    "    print(f\"Number of nodes: {graph.num_nodes}\")\n",
    "    print(f\"Number of edges: {graph.num_edges}\")\n",
    "    print(f\"Node feature dimension: {graph.num_node_features}\")\n",
    "    print(f\"Number of finemapped nodes: {data['finemapped'].sum()}\")\n",
    "\n",
    "    # Compute and print degree-related stats\n",
    "    degrees = degree(graph.edge_index[0].long(), num_nodes=graph.num_nodes)\n",
    "    average_degree = degrees.float().mean().item()\n",
    "    median_degree = np.median(degrees.numpy())\n",
    "    std_degree = degrees.float().std().item()\n",
    "\n",
    "    print(f\"Average Degree: {average_degree}\")\n",
    "    print(f\"Median Degree: {median_degree}\")\n",
    "    print(f\"Standard Deviation of Degree: {std_degree}\")\n",
    "\n",
    "    # Density is the ratio of actual edges to the maximum number of possible edges\n",
    "    num_possible_edges = graph.num_nodes * (graph.num_nodes - 1) / 2\n",
    "    density = graph.num_edges / num_possible_edges\n",
    "\n",
    "    print(f\"Density: {density:.10f}\")\n",
    "\n",
    "    # Check for NaN values in features\n",
    "    nan_mask = torch.isnan(graph.x)\n",
    "    nan_features = []\n",
    "    for feature_idx, feature_name in enumerate(features_list):\n",
    "        if nan_mask[:, feature_idx].any():\n",
    "            nan_features.append(feature_name)\n",
    "\n",
    "    print(\"Features with NaN values:\")\n",
    "    print(nan_features)\n",
    "\n",
    "def print_edge_weight_stats(edge_weights):\n",
    "    print(f\"Number of edges: {edge_weights.size(0)}\")\n",
    "    print(f\"Average edge weight: {edge_weights.float().mean().item()}\")\n",
    "    print(f\"Median edge weight: {np.median(edge_weights.numpy())}\")\n",
    "    print(f\"Standard deviation of edge weights: {edge_weights.float().std().item()}\")\n",
    "    print(f\"Maximum edge weight: {edge_weights.max().item()}\")\n",
    "    print(f\"Minimum edge weight: {edge_weights.min().item()}\")\n",
    "\n",
    "# Print graph stats\n",
    "print(\"Graph stats:\")\n",
    "print_graph_stats(graph, positive_edges_snp_snp, snp_features.columns)\n",
    "\n",
    "print(\"Edge weight stats:\")\n",
    "print_edge_weight_stats(graph.edge_attr)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9440f30f-184a-480c-bfc5-b09e76a5bee7",
   "metadata": {
    "tags": []
   },
   "source": [
    "def plot_histogram_of_edge_weights(edge_weights):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    weights = edge_weights.numpy()\n",
    "    \n",
    "    # Compute the histogram and normalize the counts\n",
    "    counts, bin_edges = np.histogram(weights, bins='auto')\n",
    "    counts = counts / counts.sum()\n",
    "\n",
    "    # Plot the normalized histogram\n",
    "    plt.bar(bin_edges[:-1], counts, width=np.diff(bin_edges), color='#0504aa', alpha=0.7, align=\"edge\")\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.xlabel('Edge Weight')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.title('Histogram of Edge Weights')\n",
    "    plt.show()\n",
    "\n",
    "# Plot histogram\n",
    "plot_histogram_of_edge_weights(graph.edge_attr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a51e96d-660c-4b4f-8ac4-9a07dba5bcc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c79fb79d-4729-4d4d-8dff-229108b6cac8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes per class in each set:\n",
      "Train set:\n",
      "Class 0: 2500 nodes\n",
      "Class 1: 2500 nodes\n",
      "Validation set:\n",
      "Class 0: 1900 nodes\n",
      "Class 1: 377 nodes\n",
      "Test set:\n",
      "Class 0: 1923 nodes\n",
      "Class 1: 354 nodes\n",
      "Percentage of Class 0 in test set: 84.45%\n",
      "Percentage of Class 1 in test set: 15.55%\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training, validation, and test sets\n",
    "transform = RandomNodeSplit(split=\"random\", num_train_per_class=2_500, num_val=0.01, num_test=0.01, key='y')\n",
    "graph = transform(graph)\n",
    "\n",
    "# Count the number of nodes per class in each set\n",
    "train_class_counts = Counter(graph.y[graph.train_mask].numpy())\n",
    "val_class_counts = Counter(graph.y[graph.val_mask].numpy())\n",
    "test_class_counts = Counter(graph.y[graph.test_mask].numpy())\n",
    "\n",
    "# Print the results\n",
    "print(\"Number of nodes per class in each set:\")\n",
    "print(\"Train set:\")\n",
    "for class_label, count in train_class_counts.items():\n",
    "    print(f\"Class {class_label}: {count} nodes\")\n",
    "print(\"Validation set:\")\n",
    "for class_label, count in val_class_counts.items():\n",
    "    print(f\"Class {class_label}: {count} nodes\")\n",
    "print(\"Test set:\")\n",
    "for class_label, count in test_class_counts.items():\n",
    "    print(f\"Class {class_label}: {count} nodes\")\n",
    "\n",
    "# Calculate and print the percentage of class 1 vs. class 0 in the test set\n",
    "total_test_nodes = sum(test_class_counts.values())\n",
    "class_0_nodes = test_class_counts[0]\n",
    "class_1_nodes = test_class_counts[1]\n",
    "class_0_percentage = (class_0_nodes / total_test_nodes) * 100\n",
    "class_1_percentage = (class_1_nodes / total_test_nodes) * 100\n",
    "print(f\"Percentage of Class 0 in test set: {class_0_percentage:.2f}%\")\n",
    "print(f\"Percentage of Class 1 in test set: {class_1_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9358aee2-41a3-4301-ab75-23ccb0ff75a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1b10db9-0e01-46ad-ab28-2b26e7fa3736",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 89.33 GiB (GPU 0; 47.99 GiB total capacity; 7.19 GiB already allocated; 33.34 GiB free; 12.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 157\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAfter early stopping, Test AUPRC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_auprc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.10f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# Call the main function\u001b[39;00m\n\u001b[1;32m--> 157\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 128\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    125\u001b[0m graph\u001b[38;5;241m.\u001b[39mtest_mask \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mtest_mask\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m150\u001b[39m):\n\u001b[1;32m--> 128\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m     _, train_acc, _ \u001b[38;5;241m=\u001b[39m evaluate(model, graph, graph\u001b[38;5;241m.\u001b[39mtrain_mask, loss_fn)\n\u001b[0;32m    130\u001b[0m     val_loss, val_acc, val_auprc \u001b[38;5;241m=\u001b[39m evaluate(model, graph, graph\u001b[38;5;241m.\u001b[39mval_mask, loss_fn)\n",
      "Cell \u001b[1;32mIn[8], line 89\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, data, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m     87\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     88\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 89\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m[data\u001b[38;5;241m.\u001b[39mtrain_mask]\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m     90\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(out, data\u001b[38;5;241m.\u001b[39my[data\u001b[38;5;241m.\u001b[39mtrain_mask]\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m     91\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[8], line 35\u001b[0m, in \u001b[0;36mGraphSAGEModel.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     33\u001b[0m x, edge_index \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m---> 35\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn_layers[i](x)\n\u001b[0;32m     37\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mleaky_relu(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch_geometric\\nn\\conv\\sage_conv.py:131\u001b[0m, in \u001b[0;36mSAGEConv.forward\u001b[1;34m(self, x, edge_index, size)\u001b[0m\n\u001b[0;32m    128\u001b[0m     x \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin(x[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mrelu(), x[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: OptPairTensor)\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_l(out)\n\u001b[0;32m    134\u001b[0m x_r \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:459\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[1;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m decomp_args:\n\u001b[0;32m    457\u001b[0m         kwargs[arg] \u001b[38;5;241m=\u001b[39m decomp_kwargs[arg][i]\n\u001b[1;32m--> 459\u001b[0m coll_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_user_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    462\u001b[0m msg_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minspector\u001b[38;5;241m.\u001b[39mdistribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m, coll_dict)\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_forward_pre_hooks\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:336\u001b[0m, in \u001b[0;36mMessagePassing._collect\u001b[1;34m(self, args, edge_index, size, kwargs)\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Tensor):\n\u001b[0;32m    335\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_size(size, dim, data)\n\u001b[1;32m--> 336\u001b[0m             data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lift\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m         out[arg] \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_sparse_tensor(edge_index):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:282\u001b[0m, in \u001b[0;36mMessagePassing._lift\u001b[1;34m(self, src, edge_index, dim)\u001b[0m\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered an index error. Please ensure that all \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindices in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m point to valid indices in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe interval [0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(got interval \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(index\u001b[38;5;241m.\u001b[39mmin())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(index\u001b[38;5;241m.\u001b[39mmax())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m])\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m index\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    286\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound negative indices in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). Please ensure that all \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindices in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m point to valid indices \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the interval [0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour node feature matrix and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:272\u001b[0m, in \u001b[0;36mMessagePassing._lift\u001b[1;34m(self, src, edge_index, dim)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    271\u001b[0m     index \u001b[38;5;241m=\u001b[39m edge_index[dim]\n\u001b[1;32m--> 272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m index\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim):\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 89.33 GiB (GPU 0; 47.99 GiB total capacity; 7.19 GiB already allocated; 33.34 GiB free; 12.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.nn.functional import binary_cross_entropy, dropout, leaky_relu\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# suppose y is your target vector\n",
    "y = graph.y.cpu().numpy()\n",
    "\n",
    "# Count number of occurrences of each class\n",
    "class_counts = np.bincount(y)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = 1. / class_counts\n",
    "\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "\n",
    "\n",
    "class GraphSAGEModel(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_layers, num_classes, dropout_rate=0.5):\n",
    "        super(GraphSAGEModel, self).__init__()\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.layers.append(SAGEConv(num_node_features, hidden_layers[0]))\n",
    "        for i in range(1, len(hidden_layers)):\n",
    "            self.layers.append(SAGEConv(hidden_layers[i - 1], hidden_layers[i]))\n",
    "        self.layers.append(SAGEConv(hidden_layers[-1], num_classes))\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.bn_layers = torch.nn.ModuleList([torch.nn.BatchNorm1d(size) for size in hidden_layers])\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        for i, conv in enumerate(self.layers[:-1]):\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.bn_layers[i](x)\n",
    "            x = torch.nn.functional.leaky_relu(x)\n",
    "            x = torch.nn.functional.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "        x = self.layers[-1](x, edge_index)\n",
    "        return torch.sigmoid(x.view(-1))\n",
    "    \n",
    "    \n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets.long()].view(-1, 1)\n",
    "            logpt = -binary_cross_entropy(inputs, targets, reduction='none')\n",
    "            logpt = logpt * alpha_t\n",
    "        else:\n",
    "            logpt = -binary_cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(logpt)\n",
    "        F_loss = -((1 - pt) ** self.gamma) * logpt\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "\n",
    "        \n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def step(self, val_loss):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\\\n",
    "            \n",
    "            \n",
    "def train(model, data, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)[data.train_mask].squeeze()\n",
    "    loss = loss_fn(out, data.y[data.train_mask].float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def evaluate(model, data, mask, loss_fn):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)[mask].squeeze()\n",
    "        preds = (out > 0.5).long()\n",
    "        loss = loss_fn(out, data.y[mask].float())\n",
    "        accuracy = preds.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        precision, recall, _ = precision_recall_curve(data.y[mask].cpu(), out.cpu())\n",
    "        auprc = auc(recall, precision)\n",
    "    return loss.item(), accuracy, auprc\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # initialization\n",
    "    hidden_layers = [64, 64] \n",
    "    model = GraphSAGEModel(graph.num_node_features, hidden_layers, 1, dropout_rate=0.5).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)\n",
    "    loss_fn = FocalLoss(alpha=class_weights, gamma=2)\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=10, min_delta=0.0001)\n",
    "\n",
    "    # Move your graph data to device\n",
    "    graph.x = graph.x.to(device)\n",
    "    graph.edge_index = graph.edge_index.to(device)\n",
    "    graph.y = graph.y.to(device)\n",
    "    graph.train_mask = graph.train_mask.to(device)\n",
    "    graph.val_mask = graph.val_mask.to(device)\n",
    "    graph.test_mask = graph.test_mask.to(device)\n",
    "\n",
    "    for epoch in range(150):\n",
    "        train_loss = train(model, graph, loss_fn, optimizer)\n",
    "        _, train_acc, _ = evaluate(model, graph, graph.train_mask, loss_fn)\n",
    "        val_loss, val_acc, val_auprc = evaluate(model, graph, graph.val_mask, loss_fn)\n",
    "        print(f'Epoch: {epoch+1}, Loss: {train_loss:.10f}, Train Acc: {train_acc:.10f}, Val Loss: {val_loss:.10f}, Val Acc: {val_acc:.10f}, Val AUPRC: {val_auprc:.10f}')\n",
    "\n",
    "        # Reduce learning rate when validation loss plateaus\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping.step(-val_auprc)  # Pass -val_auprc because we want to maximize it\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "        # Save checkpoint if is a new best\n",
    "        if -val_auprc == early_stopping.best_score:  # Save the best model based on AUPRC\n",
    "            torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "\n",
    "    # Load the best model back in\n",
    "    if os.path.isfile('checkpoint.pt'):\n",
    "        model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    else:\n",
    "        print(\"No checkpoint found. Creating a new file checkpoint.pt.\")\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        \n",
    "    test_loss, test_acc, test_auprc = evaluate(model, graph, graph.test_mask, loss_fn)\n",
    "    print(f'After early stopping, Test AUPRC: {test_auprc:.10f}')\n",
    "\n",
    "# Call the main function\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
