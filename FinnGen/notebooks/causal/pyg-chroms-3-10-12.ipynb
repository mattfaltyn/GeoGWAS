{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9621723b-7df6-4c61-b4d8-9762c18ef090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.5 (tags/v3.11.5:cce6ba9, Aug 24 2023, 14:38:34) [MSC v.1936 64 bit (AMD64)]\n",
      "NumPy version: 1.24.1\n",
      "Pandas version: 2.1.0\n",
      "Matplotlib version: 3.7.2\n",
      "Scikit-learn version: 1.3.0\n",
      "Torch version: 2.0.1+cu117\n",
      "Torch Geometric version: 2.3.1\n",
      "NetworkX version: 3.0\n",
      "Using NVIDIA RTX A6000 (cuda)\n",
      "CUDA version: 11.7\n",
      "Number of CUDA devices: 2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv\n",
    "from torch_geometric.utils import to_undirected, negative_sampling\n",
    "import networkx as nx\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.special import expit\n",
    "from typing import List, Dict\n",
    "import time\n",
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "import category_encoders as ce\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "\n",
    "# Print versions of imported libraries\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Matplotlib version: {matplotlib.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Torch Geometric version: {torch_geometric.__version__}\")\n",
    "print(f\"NetworkX version: {nx.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Current CUDA device\n",
    "    print(f\"Using {torch.cuda.get_device_name()} ({device})\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75db906-087f-4778-8c06-21ac56bd0487",
   "metadata": {},
   "source": [
    "## Spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319c409b-54ff-4aae-b483-92be6d3b198f",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "`data` Pandas DataFrame:\n",
    "\n",
    "- `#chrom`: chromosome of SNP (int).\n",
    "- `id`: the ID of the variant in the following format: `#chrom:pos:ref:alt` (string).\n",
    "- `pos`: position of the genetic variant on the chromosome (int).\n",
    "- `ref`: reference allele (or variant) at the genomic position (string).\n",
    "- `alt`: alternate allele observed at this position (string).\n",
    "- `gene_0` to `gene_21`: genes which are nearest to the variant (string).\n",
    "- `mlogp`: minus log of the p-value, commonly used in genomic studies (float).\n",
    "- `beta`: beta coefficient represents the effect size of the variant (float).\n",
    "- `sebeta`: standard error of the beta coefficient (float).\n",
    "- `af_alt`: allele frequency of the alternate variant in the general population (float).\n",
    "- `af_alt_cases`: allele frequency of the alternate variant in the cases group (float).\n",
    "- `af_alt_controls`: allele frequency of the alternate variant in the control group (float).\n",
    "- `prob`: posterior probability of association (float).\n",
    "- `lead_r2`: r2 value to a lead variant (the one with maximum PIP) in a credible set (float).\n",
    "- `cs_99`: credible set to which the variant belongs to (int).\n",
    "- `causal`: indicates causality of variant (1) or not (0) (int). \n",
    "\n",
    "### Task Overview\n",
    "\n",
    "The objective is to design and implement a binary node classification GNN model to predict whether variants are causal (`causal=1`) or not (`causal=0`).\n",
    "\n",
    "### Nodes and Their Features\n",
    "\n",
    "**SNP Nodes**: Each SNP Node is characterized by various features:\n",
    "`['#chrom', 'pos', 'ref', 'alt', 'mlogp', 'beta', 'sebeta', 'af_alt', 'af_alt_cases', 'prob', 'lead_r2', 'cs_99']`.\n",
    "\n",
    "\n",
    "### Edges\n",
    "\n",
    "Edges are created between 2 variants if either 1) the SNPs share the same nearest gene value (i.e., they share a value across the `gene_0` to `gene_21` columns, OR 2) the SNPs are on the same `#chrom` and share the same `cs_99` value and both have `lead_r2 > 0.8`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7273a26-b897-47b0-b802-7e9d6cecd7d0",
   "metadata": {},
   "source": [
    "## Graph Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f3b90b6-363a-449b-ae76-a0938065e32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the data\n",
    "data = pd.read_parquet('gwas_fm_t2d.parquet')\n",
    "\n",
    "# Process only chromosome 10 and 3\n",
    "chroms = [3, 10, 12]\n",
    "\n",
    "data = data[data['#chrom'].isin(chroms)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9679b8a-1d5f-4994-948c-7060b6d05f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\category_encoders\\utils.py:28: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif pd.api.types.is_categorical_dtype(cols):\n",
      "C:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\category_encoders\\utils.py:50: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return pd.api.types.is_categorical_dtype(dtype)\n",
      "C:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\category_encoders\\utils.py:50: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return pd.api.types.is_categorical_dtype(dtype)\n",
      "C:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\category_encoders\\utils.py:50: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return pd.api.types.is_categorical_dtype(dtype)\n",
      "C:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\category_encoders\\utils.py:50: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return pd.api.types.is_categorical_dtype(dtype)\n",
      "C:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\category_encoders\\utils.py:50: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return pd.api.types.is_categorical_dtype(dtype)\n",
      "C:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\category_encoders\\utils.py:50: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return pd.api.types.is_categorical_dtype(dtype)\n",
      "C:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\category_encoders\\utils.py:50: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return pd.api.types.is_categorical_dtype(dtype)\n",
      "C:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\category_encoders\\utils.py:50: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return pd.api.types.is_categorical_dtype(dtype)\n",
      "C:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\category_encoders\\utils.py:50: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return pd.api.types.is_categorical_dtype(dtype)\n",
      "C:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\category_encoders\\utils.py:50: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return pd.api.types.is_categorical_dtype(dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 15075\n",
      "Number of edges: 2547943\n",
      "Node feature dimension: 11\n",
      "Execution time: 1.3379998207092285 seconds\n",
      "         72155 function calls (70728 primitive calls) in 1.150 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 1015 to 3 due to restriction <3>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "       13    0.000    0.000    1.335    0.103 C:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3472(run_code)\n",
      "       13    0.000    0.000    1.335    0.103 {built-in method builtins.exec}\n",
      "        1    0.330    0.330    1.098    1.098 C:\\Users\\Windows\\AppData\\Local\\Temp\\ipykernel_27356\\4043721445.py:19(preprocess_edges)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_unique_snps(data: pd.DataFrame) -> dict:\n",
    "    return {snp: idx for idx, snp in enumerate(data['id'].unique())}\n",
    "\n",
    "def preprocess_snp_features(data: pd.DataFrame, snp_to_idx: dict) -> pd.DataFrame:\n",
    "    cols_to_extract = ['id', '#chrom', 'pos', 'ref', 'alt', 'mlogp', 'beta', 'sebeta', \n",
    "                       'af_alt', 'af_alt_cases', 'af_alt_controls', 'prob']\n",
    "    snp_features = data.loc[data['id'].isin(snp_to_idx.keys()), cols_to_extract].set_index('id').sort_index()\n",
    "    \n",
    "    categorical_cols = ['ref', 'alt']\n",
    "    count_encoder = ce.CountEncoder(cols=categorical_cols)\n",
    "    snp_features = count_encoder.fit_transform(snp_features)\n",
    "\n",
    "    numerical_cols = list(set(snp_features.columns) - set(categorical_cols))\n",
    "\n",
    "    snp_features = snp_features.fillna(0)\n",
    "\n",
    "    return snp_features\n",
    "\n",
    "def preprocess_edges(data: pd.DataFrame, snp_to_idx: dict) -> torch.Tensor:\n",
    "    data['snp_idx'] = data['id'].map(snp_to_idx)\n",
    "\n",
    "    # Create a mapping from gene values to node indices for each gene column\n",
    "    gene_cols = [f'gene_{i}' for i in range(22)]\n",
    "    melted_df = data.melt(id_vars='snp_idx', value_vars=gene_cols)\n",
    "    grouped_df = melted_df[melted_df['value'].notnull() & (melted_df['value'] != 0)].groupby('value')['snp_idx'].apply(list)\n",
    "\n",
    "    # Create edges based on shared nearest gene values\n",
    "    edge_list = [edge for node_indices in grouped_df for edge in combinations(node_indices, 2)]\n",
    "\n",
    "    # Filter data outside the loop to reduce the DataFrame size\n",
    "    filtered_df = data[data['lead_r2'] > 0.8][['id', 'cs_99', '#chrom']]\n",
    "    filtered_data_list = filtered_df.to_records(index=False).tolist()\n",
    "\n",
    "    # Create edges based on lead_r2 values and same chromosome\n",
    "    for i, (id_i, cs_99_i, chrom_i) in enumerate(filtered_data_list):\n",
    "        for id_j, cs_99_j, chrom_j in filtered_data_list[i + 1:]:\n",
    "            if cs_99_i == cs_99_j and chrom_i == chrom_j:\n",
    "                edge_list.append((snp_to_idx[id_i], snp_to_idx[id_j]))\n",
    "\n",
    "    # Remove duplicate edges\n",
    "    edge_list = list(set(edge_list))\n",
    "    edge_tensor = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    return edge_tensor\n",
    "\n",
    "\n",
    "def create_pytorch_graph(features: torch.Tensor, edges: torch.Tensor, edge_attr: torch.Tensor) -> Data:\n",
    "    return Data(x=features, edge_index=edges, edge_attr=edge_attr)\n",
    "\n",
    "# Create a profiler object\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "snp_to_idx = get_unique_snps(data)\n",
    "snp_features = preprocess_snp_features(data, snp_to_idx)\n",
    "features = torch.tensor(snp_features.values, dtype=torch.float)\n",
    "\n",
    "edges = preprocess_edges(data, snp_to_idx)\n",
    "graph = create_pytorch_graph(features, edges, None)  # Assuming no edge attributes for simplicity\n",
    "graph.y = torch.tensor(data['causal'].values, dtype=torch.long)\n",
    "\n",
    "print(f\"Number of nodes: {graph.num_nodes}\")\n",
    "print(f\"Number of edges: {graph.num_edges}\")\n",
    "print(f\"Node feature dimension: {graph.num_node_features}\")\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Execution time: {elapsed_time} seconds\")\n",
    "\n",
    "pr.disable()\n",
    "s = io.StringIO()\n",
    "sortby = 'cumulative'\n",
    "ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "ps.print_stats(3)  # Only print the top 3 lines\n",
    "print(s.getvalue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9a104fa-b125-4758-9f3f-0d820e7c6968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum node degree: 1\n",
      "Average node degree: 171.22\n",
      "Median node degree: 85.0\n",
      "Node with the highest degree (1411): 8726\n",
      "Degree distribution (Degree: Count) -> Counter({1: 188, 2: 174, 3: 158, 4: 149, 5: 145, 6: 138, 8: 134, 7: 134, 9: 129, 10: 127, 11: 125, 12: 124, 13: 122, 15: 121, 14: 120, 17: 118, 16: 118, 18: 115, 20: 113, 19: 111, 22: 108, 21: 108, 25: 104, 24: 104, 23: 104, 27: 99, 26: 98, 28: 94, 29: 92, 31: 92, 30: 92, 38: 88, 32: 88, 35: 87, 34: 87, 33: 87, 39: 87, 37: 86, 41: 85, 36: 82, 40: 82, 43: 79, 45: 78, 44: 78, 42: 78, 46: 76, 49: 75, 47: 75, 50: 74, 48: 74, 51: 73, 52: 72, 55: 71, 54: 71, 53: 71, 56: 70, 57: 68, 58: 67, 59: 66, 62: 65, 60: 64, 61: 64, 63: 64, 64: 63, 65: 61, 70: 61, 66: 60, 71: 60, 68: 60, 73: 60, 72: 60, 67: 60, 69: 59, 77: 59, 78: 59, 75: 57, 76: 57, 80: 57, 74: 57, 81: 56, 84: 55, 82: 55, 79: 54, 83: 54, 85: 53, 86: 52, 89: 51, 92: 51, 87: 51, 95: 51, 100: 50, 101: 50, 93: 50, 90: 50, 94: 50, 104: 50, 88: 50, 97: 49, 103: 49, 91: 49, 96: 49, 98: 49, 110: 48, 106: 48, 108: 48, 99: 48, 102: 48, 109: 47, 107: 47, 111: 47, 105: 47, 112: 45, 114: 44, 117: 44, 115: 44, 119: 44, 118: 44, 120: 44, 113: 44, 116: 44, 121: 43, 123: 42, 122: 42, 124: 40, 125: 39, 126: 38, 128: 38, 127: 38, 129: 35, 130: 35, 131: 34, 133: 33, 135: 33, 132: 33, 134: 33, 148: 32, 147: 32, 146: 32, 137: 32, 142: 32, 145: 32, 136: 32, 143: 32, 144: 32, 141: 32, 139: 32, 138: 32, 140: 32, 149: 31, 150: 31, 155: 29, 156: 29, 157: 29, 151: 29, 152: 29, 153: 29, 154: 29, 159: 27, 158: 27, 160: 27, 168: 26, 162: 26, 164: 26, 169: 26, 163: 26, 161: 26, 166: 26, 167: 26, 165: 26, 174: 25, 173: 25, 170: 25, 171: 25, 172: 25, 175: 24, 176: 24, 177: 24, 178: 23, 182: 22, 181: 22, 180: 22, 179: 22, 183: 21, 184: 21, 185: 20, 186: 20, 187: 20, 193: 19, 189: 19, 190: 19, 191: 19, 192: 19, 188: 19, 196: 19, 197: 18, 194: 18, 199: 18, 198: 18, 200: 18, 195: 18, 203: 17, 202: 17, 201: 17, 204: 16, 216: 16, 205: 16, 214: 15, 206: 15, 215: 15, 210: 15, 208: 15, 213: 15, 207: 15, 211: 15, 212: 15, 209: 15, 217: 14, 219: 14, 218: 14, 220: 14, 223: 13, 225: 13, 227: 13, 222: 13, 221: 13, 226: 13, 228: 13, 224: 13, 229: 12, 234: 12, 253: 12, 245: 12, 250: 12, 249: 12, 233: 12, 251: 12, 248: 12, 242: 12, 235: 12, 247: 12, 244: 12, 238: 12, 243: 12, 241: 12, 230: 12, 232: 12, 239: 12, 237: 12, 231: 12, 240: 12, 236: 12, 252: 12, 246: 12, 265: 11, 259: 11, 256: 11, 257: 11, 261: 11, 267: 11, 263: 11, 264: 11, 255: 11, 262: 11, 266: 11, 254: 11, 260: 11, 268: 11, 274: 10, 270: 10, 272: 10, 273: 10, 258: 10, 271: 10, 275: 10, 269: 10, 281: 9, 290: 9, 280: 9, 316: 9, 289: 9, 288: 9, 286: 9, 312: 9, 309: 9, 292: 9, 314: 9, 278: 9, 300: 9, 284: 9, 298: 9, 285: 9, 306: 9, 296: 9, 307: 9, 276: 9, 299: 9, 297: 9, 295: 9, 308: 9, 293: 9, 291: 9, 304: 9, 283: 9, 313: 9, 310: 9, 294: 9, 305: 9, 282: 9, 302: 9, 287: 9, 301: 9, 311: 9, 315: 9, 279: 9, 277: 9, 303: 9, 355: 8, 317: 8, 318: 8, 319: 8, 324: 7, 339: 7, 330: 7, 343: 7, 351: 7, 349: 7, 352: 7, 348: 7, 347: 7, 331: 7, 356: 7, 340: 7, 359: 7, 344: 7, 357: 7, 320: 7, 345: 7, 326: 7, 335: 7, 327: 7, 354: 7, 353: 7, 336: 7, 341: 7, 322: 7, 333: 7, 350: 7, 321: 7, 361: 7, 338: 7, 337: 7, 329: 7, 332: 7, 346: 7, 325: 7, 328: 7, 323: 7, 358: 7, 360: 7, 371: 6, 382: 6, 367: 6, 376: 6, 375: 6, 378: 6, 379: 6, 387: 6, 385: 6, 363: 6, 384: 6, 373: 6, 386: 6, 342: 6, 372: 6, 383: 6, 381: 6, 380: 6, 369: 6, 439: 6, 368: 6, 377: 6, 431: 6, 374: 6, 364: 6, 370: 6, 365: 6, 334: 6, 366: 6, 362: 6, 426: 5, 453: 5, 489: 5, 396: 5, 488: 5, 486: 5, 418: 5, 432: 5, 395: 5, 422: 5, 406: 5, 485: 5, 455: 5, 445: 5, 401: 5, 430: 5, 398: 5, 491: 5, 454: 5, 399: 5, 410: 5, 423: 5, 411: 5, 483: 5, 458: 5, 390: 5, 482: 5, 481: 5, 427: 5, 449: 5, 428: 5, 402: 5, 397: 5, 473: 5, 487: 5, 450: 5, 477: 5, 456: 5, 419: 5, 446: 5, 407: 5, 479: 5, 425: 5, 413: 5, 478: 5, 466: 5, 391: 5, 484: 5, 424: 5, 414: 5, 434: 5, 452: 5, 442: 5, 440: 5, 403: 5, 409: 5, 490: 5, 474: 5, 462: 5, 480: 5, 468: 5, 437: 5, 415: 5, 435: 5, 469: 5, 448: 5, 438: 5, 436: 5, 405: 5, 404: 5, 476: 5, 464: 5, 463: 5, 433: 5, 408: 5, 470: 5, 400: 5, 460: 5, 392: 5, 459: 5, 429: 5, 467: 5, 451: 5, 465: 5, 447: 5, 461: 5, 420: 5, 388: 5, 457: 5, 416: 5, 475: 5, 444: 5, 443: 5, 389: 5, 412: 5, 471: 5, 472: 5, 441: 5, 393: 5, 421: 5, 394: 5, 417: 5, 498: 4, 524: 4, 512: 4, 493: 4, 526: 4, 516: 4, 514: 4, 513: 4, 520: 4, 533: 4, 521: 4, 509: 4, 517: 4, 515: 4, 505: 4, 494: 4, 525: 4, 523: 4, 532: 4, 511: 4, 501: 4, 529: 4, 508: 4, 519: 4, 496: 4, 528: 4, 507: 4, 497: 4, 495: 4, 504: 4, 522: 4, 492: 4, 506: 4, 531: 4, 500: 4, 518: 4, 510: 4, 530: 4, 503: 4, 502: 4, 527: 4, 499: 4, 568: 3, 538: 3, 560: 3, 557: 3, 627: 3, 583: 3, 545: 3, 597: 3, 571: 3, 584: 3, 565: 3, 573: 3, 536: 3, 554: 3, 572: 3, 624: 3, 542: 3, 564: 3, 541: 3, 552: 3, 556: 3, 570: 3, 544: 3, 567: 3, 580: 3, 550: 3, 598: 3, 561: 3, 586: 3, 549: 3, 595: 3, 620: 3, 574: 3, 537: 3, 566: 3, 540: 3, 535: 3, 588: 3, 576: 3, 622: 3, 582: 3, 563: 3, 551: 3, 539: 3, 579: 3, 590: 3, 578: 3, 547: 3, 575: 3, 534: 3, 628: 3, 593: 3, 543: 3, 558: 3, 577: 3, 617: 3, 569: 3, 546: 3, 585: 3, 591: 3, 548: 3, 581: 3, 562: 3, 587: 3, 559: 3, 553: 3, 555: 3, 614: 2, 596: 2, 642: 2, 643: 2, 599: 2, 612: 2, 631: 2, 623: 2, 615: 2, 592: 2, 639: 2, 608: 2, 852: 2, 611: 2, 636: 2, 647: 2, 635: 2, 616: 2, 604: 2, 619: 2, 632: 2, 600: 2, 648: 2, 605: 2, 601: 2, 589: 2, 629: 2, 644: 2, 625: 2, 613: 2, 641: 2, 640: 2, 621: 2, 609: 2, 637: 2, 610: 2, 645: 2, 633: 2, 638: 2, 606: 2, 594: 2, 634: 2, 603: 2, 618: 2, 602: 2, 646: 2, 630: 2, 626: 2, 607: 2, 1343: 1, 753: 1, 1056: 1, 855: 1, 1229: 1, 1331: 1, 741: 1, 843: 1, 895: 1, 997: 1, 1319: 1, 1371: 1, 831: 1, 883: 1, 985: 1, 1037: 1, 1139: 1, 973: 1, 1127: 1, 1300: 1, 812: 1, 1115: 1, 1288: 1, 1390: 1, 800: 1, 902: 1, 954: 1, 1108: 1, 1378: 1, 890: 1, 942: 1, 1044: 1, 1096: 1, 1198: 1, 1032: 1, 1084: 1, 1186: 1, 1340: 1, 1359: 1, 1072: 1, 871: 1, 1174: 1, 1025: 1, 1347: 1, 780: 1, 859: 1, 1013: 1, 1167: 1, 1001: 1, 1103: 1, 1155: 1, 1257: 1, 1309: 1, 1091: 1, 1143: 1, 1245: 1, 1399: 1, 1339: 1, 655: 1, 749: 1, 851: 1, 1005: 1, 828: 1, 1233: 1, 1387: 1, 1406: 1, 1095: 1, 918: 1, 970: 1, 1375: 1, 1226: 1, 1328: 1, 678: 1, 1060: 1, 1112: 1, 1162: 1, 1214: 1, 1316: 1, 1368: 1, 1256: 1, 1410: 1, 726: 1, 768: 1, 820: 1, 922: 1, 1100: 1, 1202: 1, 1304: 1, 1356: 1, 1398: 1, 714: 1, 868: 1, 808: 1, 910: 1, 1064: 1, 887: 1, 1166: 1, 1041: 1, 1052: 1, 1029: 1, 1308: 1, 1131: 1, 1183: 1, 1285: 1, 1225: 1, 1327: 1, 737: 1, 1040: 1, 839: 1, 891: 1, 993: 1, 1119: 1, 1171: 1, 1273: 1, 1325: 1, 1315: 1, 1367: 1, 683: 1, 785: 1, 827: 1, 879: 1, 981: 1, 1033: 1, 1135: 1, 1261: 1, 773: 1, 927: 1, 969: 1, 1021: 1, 1123: 1, 1277: 1, 1403: 1, 1254: 1, 1296: 1, 1009: 1, 1111: 1, 1088: 1, 1190: 1, 1242: 1, 1344: 1, 1396: 1, 1284: 1, 702: 1, 754: 1, 796: 1, 950: 1, 1178: 1, 1332: 1, 844: 1, 938: 1, 1092: 1, 1194: 1, 1246: 1, 1028: 1, 1080: 1, 1182: 1, 1336: 1, 1313: 1, 1068: 1, 1170: 1, 1324: 1, 1163: 1, 1099: 1, 1151: 1, 1253: 1, 1305: 1, 1241: 1, 1293: 1, 1395: 1, 651: 1, 824: 1, 1383: 1, 966: 1, 1222: 1, 1210: 1, 1312: 1, 1364: 1, 1352: 1, 710: 1, 864: 1, 698: 1, 1179: 1, 1281: 1, 1269: 1, 1321: 1, 679: 1, 781: 1, 1411: 1, 667: 1, 769: 1, 821: 1, 923: 1, 1250: 1, 911: 1, 1238: 1, 1392: 1, 1380: 1, 738: 1, 840: 1, 892: 1, 880: 1, 982: 1, 1076: 1, 1230: 1, 1124: 1, 1320: 1, 1297: 1, 707: 1, 809: 1, 1159: 1, 695: 1, 849: 1, 797: 1, 951: 1, 1147: 1, 1249: 1, 1301: 1, 837: 1, 939: 1, 1093: 1, 1237: 1, 1289: 1, 1391: 1, 778: 1, 1379: 1, 789: 1, 766: 1, 920: 1, 962: 1, 1218: 1, 856: 1, 908: 1, 1010: 1, 1104: 1, 1206: 1, 1360: 1, 998: 1, 1152: 1, 1348: 1, 706: 1, 758: 1, 694: 1, 848: 1, 979: 1, 1175: 1, 836: 1, 1069: 1, 1223: 1, 1265: 1, 1317: 1, 675: 1, 777: 1, 1407: 1, 663: 1, 765: 1, 817: 1, 919: 1, 805: 1, 907: 1, 1061: 1, 1234: 1, 1388: 1, 1376: 1, 734: 1, 722: 1, 774: 1, 876: 1, 978: 1, 1120: 1, 703: 1, 691: 1, 793: 1, 845: 1, 947: 1, 833: 1, 935: 1, 1089: 1, 1077: 1, 762: 1, 916: 1, 750: 1, 904: 1, 1006: 1, 1058: 1, 994: 1, 1148: 1, 1136: 1, 690: 1, 742: 1, 975: 1, 832: 1, 963: 1, 1065: 1, 1117: 1, 671: 1, 1053: 1, 1105: 1, 1207: 1, 1361: 1, 659: 1, 761: 1, 813: 1, 915: 1, 1195: 1, 801: 1, 903: 1, 1057: 1, 1034: 1, 1384: 1, 1045: 1, 1022: 1, 1176: 1, 1278: 1, 1372: 1, 730: 1, 1062: 1, 1164: 1, 1266: 1, 676: 1, 718: 1, 770: 1, 872: 1, 974: 1, 1408: 1, 664: 1, 860: 1, 1014: 1, 1116: 1, 991: 1, 699: 1, 1081: 1, 1133: 1, 1235: 1, 1337: 1, 1389: 1, 687: 1, 841: 1, 943: 1, 1377: 1, 829: 1, 931: 1, 1085: 1, 1073: 1, 1306: 1, 746: 1, 900: 1, 1002: 1, 1054: 1, 888: 1, 990: 1, 1042: 1, 1144: 1, 1030: 1, 1132: 1, 971: 1, 959: 1, 1113: 1, 1049: 1, 1101: 1, 1203: 1, 1191: 1, 1345: 1, 1018: 1, 1172: 1, 1274: 1, 1160: 1, 1262: 1, 1314: 1, 1404: 1, 660: 1, 987: 1, 1129: 1, 1231: 1, 1333: 1, 1219: 1, 1373: 1, 731: 1, 825: 1, 719: 1, 873: 1, 1046: 1, 1200: 1, 1302: 1, 1188: 1, 1290: 1, 1342: 1, 700: 1, 896: 1, 1330: 1, 688: 1, 790: 1, 842: 1, 884: 1, 986: 1, 1038: 1, 1140: 1, 932: 1, 1026: 1, 1128: 1, 1282: 1, 1259: 1, 967: 1, 1247: 1, 1349: 1, 1401: 1, 657: 1, 759: 1, 955: 1, 1109: 1, 747: 1, 901: 1, 995: 1, 1097: 1, 1199: 1, 1251: 1, 889: 1, 1145: 1, 1187: 1, 1341: 1, 1318: 1, 728: 1, 924: 1, 1329: 1, 818: 1, 912: 1, 1066: 1, 1168: 1, 1156: 1, 1258: 1, 1310: 1, 1298: 1, 1400: 1, 656: 1, 983: 1, 1137: 1, 1125: 1, 1227: 1, 1279: 1, 1215: 1, 1267: 1, 1369: 1, 727: 1, 1357: 1, 715: 1, 869: 1, 1196: 1, 857: 1, 1184: 1, 1286: 1, 1338: 1, 1326: 1, 684: 1, 786: 1, 838: 1, 672: 1, 826: 1, 928: 1, 1255: 1, 1243: 1, 1397: 1, 653: 1, 755: 1, 1385: 1, 743: 1, 795: 1, 897: 1, 885: 1, 712: 1, 814: 1, 866: 1, 802: 1, 854: 1, 956: 1, 1050: 1, 1204: 1, 944: 1, 1098: 1, 1294: 1, 1271: 1, 652: 1, 783: 1, 794: 1, 771: 1, 925: 1, 1121: 1, 1275: 1, 861: 1, 913: 1, 1015: 1, 1067: 1, 1211: 1, 1263: 1, 1365: 1, 723: 1, 1003: 1, 1157: 1, 1353: 1, 711: 1, 763: 1, 865: 1, 1192: 1, 853: 1, 830: 1, 984: 1, 1086: 1, 1180: 1, 1334: 1, 972: 1, 1074: 1, 1126: 1, 1228: 1, 1270: 1, 1322: 1, 680: 1, 732: 1, 782: 1, 1216: 1, 668: 1, 822: 1, 799: 1, 810: 1, 1043: 1, 1197: 1, 1239: 1, 1393: 1, 649: 1, 751: 1, 1287: 1, 1381: 1, 739: 1, 791: 1, 893: 1, 779: 1, 881: 1, 1035: 1, 1208: 1, 1350: 1, 708: 1, 696: 1, 798: 1, 952: 1, 940: 1, 1094: 1, 1082: 1, 1409: 1, 767: 1, 921: 1, 909: 1, 1011: 1, 1063: 1, 999: 1, 1051: 1, 1153: 1, 1141: 1, 724: 1, 980: 1, 968: 1, 1070: 1, 1122: 1, 1224: 1, 1110: 1, 1212: 1, 1366: 1, 716: 1, 1354: 1, 806: 1, 937: 1, 1039: 1, 1027: 1, 1079: 1, 1181: 1, 1283: 1, 1335: 1, 735: 1, 787: 1, 1169: 1, 681: 1, 775: 1, 877: 1, 1031: 1, 1008: 1, 669: 1, 1019: 1, 996: 1, 1150: 1, 1252: 1, 1346: 1, 704: 1, 1138: 1, 1240: 1, 1394: 1, 650: 1, 692: 1, 846: 1, 948: 1, 1382: 1, 740: 1, 834: 1, 988: 1, 936: 1, 1090: 1, 1370: 1, 882: 1, 1078: 1, 1055: 1, 1209: 1, 1311: 1, 1363: 1, 1405: 1, 661: 1, 917: 1, 1299: 1, 1351: 1, 709: 1, 803: 1, 905: 1, 1007: 1, 1059: 1, 1047: 1, 1149: 1, 1280: 1, 1268: 1, 720: 1, 874: 1, 976: 1, 1358: 1, 862: 1, 964: 1, 1016: 1, 1118: 1, 1004: 1, 1106: 1, 1362: 1, 945: 1, 933: 1, 1087: 1, 1023: 1, 1075: 1, 1177: 1, 1165: 1, 677: 1, 665: 1, 992: 1, 1146: 1, 1248: 1, 1134: 1, 1236: 1, 736: 1, 878: 1, 1205: 1, 1307: 1, 1193: 1, 1295: 1, 705: 1, 693: 1, 847: 1, 1276: 1, 1264: 1, 674: 1, 870: 1, 662: 1, 764: 1, 816: 1, 858: 1, 960: 1, 1012: 1, 1114: 1, 752: 1, 906: 1, 1000: 1, 1102: 1, 894: 1, 941: 1, 1221: 1, 1323: 1, 733: 1, 929: 1, 1083: 1, 721: 1, 823: 1, 875: 1, 1071: 1, 1173: 1, 811: 1, 863: 1, 965: 1, 1161: 1, 1292: 1, 673: 1, 953: 1, 1303: 1, 815: 1, 792: 1, 1291: 1, 1142: 1, 934: 1, 1036: 1, 1130: 1, 1232: 1, 1386: 1, 1024: 1, 1220: 1, 1272: 1, 1374: 1, 1201: 1, 1189: 1, 701: 1, 689: 1, 1158: 1, 1260: 1, 819: 1, 1402: 1, 658: 1, 760: 1, 748: 1, 1217: 1, 729: 1, 717: 1, 807: 1, 961: 1, 949: 1, 686: 1, 788: 1, 776: 1, 930: 1, 1020: 1, 757: 1, 745: 1, 899: 1, 835: 1, 989: 1, 1185: 1, 697: 1, 977: 1, 685: 1, 804: 1, 958: 1, 1154: 1, 946: 1, 1048: 1, 1244: 1, 654: 1, 756: 1, 744: 1, 898: 1, 784: 1, 886: 1, 1017: 1, 1213: 1, 725: 1, 1107: 1, 1355: 1, 713: 1, 867: 1, 957: 1, 682: 1, 670: 1, 772: 1, 926: 1, 914: 1, 666: 1})\n",
      "Node degree variance: 59596.96\n",
      "Number of nodes with a degree above average: 4136\n",
      "Number of nodes with a degree below average: 10745\n",
      "Number of isolated nodes: 194\n",
      "Total number of edges: 2547943\n",
      "Number of unique edges: 2547943\n",
      "Number of self-loops: 0\n",
      "Number of multiple edges (duplicates): 0\n",
      "Graph density: 0.0112\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Calculate new graph statistics:\n",
    "\n",
    "# Node degrees:\n",
    "degrees = Counter(graph.edge_index[0].numpy())\n",
    "avg_degree = sum(degrees.values()) / len(degrees)\n",
    "\n",
    "# Minimum, Maximum, and Median Degree:\n",
    "min_degree = min(degrees.values())\n",
    "max_degree_node = max(degrees, key=degrees.get)\n",
    "median_degree = np.median(list(degrees.values()))\n",
    "\n",
    "print(f\"Minimum node degree: {min_degree}\")\n",
    "print(f\"Average node degree: {avg_degree:.2f}\")\n",
    "print(f\"Median node degree: {median_degree}\")\n",
    "print(f\"Node with the highest degree ({degrees[max_degree_node]}): {max_degree_node}\")\n",
    "\n",
    "# Degree Distribution:\n",
    "degree_values = list(degrees.values())\n",
    "degree_count = Counter(degree_values)\n",
    "print(f\"Degree distribution (Degree: Count) -> {degree_count}\")\n",
    "\n",
    "# Degree Variance:\n",
    "degree_variance = np.var(degree_values)\n",
    "print(f\"Node degree variance: {degree_variance:.2f}\")\n",
    "\n",
    "# Nodes with Degree Above/Below Average:\n",
    "above_avg_count = sum(1 for degree in degree_values if degree > avg_degree)\n",
    "below_avg_count = sum(1 for degree in degree_values if degree < avg_degree)\n",
    "print(f\"Number of nodes with a degree above average: {above_avg_count}\")\n",
    "print(f\"Number of nodes with a degree below average: {below_avg_count}\")\n",
    "\n",
    "# Number of Isolated Nodes:\n",
    "isolated_nodes = graph.num_nodes - len(degrees)\n",
    "print(f\"Number of isolated nodes: {isolated_nodes}\")\n",
    "\n",
    "# Edge statistics:\n",
    "\n",
    "# Total Edge Count:\n",
    "print(f\"Total number of edges: {graph.num_edges}\")\n",
    "\n",
    "# Unique Edges:\n",
    "unique_edges = set(tuple(edge) for edge in graph.edge_index.t().numpy())\n",
    "print(f\"Number of unique edges: {len(unique_edges)}\")\n",
    "\n",
    "# Self-loops:\n",
    "self_loops = (graph.edge_index[0] == graph.edge_index[1]).sum().item()\n",
    "print(f\"Number of self-loops: {self_loops}\")\n",
    "\n",
    "# Multiple Edges:\n",
    "multiple_edges = graph.num_edges - len(unique_edges) + self_loops\n",
    "print(f\"Number of multiple edges (duplicates): {multiple_edges}\")\n",
    "\n",
    "# Graph Density:\n",
    "max_possible_edges = graph.num_nodes * (graph.num_nodes - 1)\n",
    "density = graph.num_edges / max_possible_edges\n",
    "print(f\"Graph density: {density:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcb0a3c-21ef-4b53-8c3f-974067c89b13",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e7d59f9-aa8e-4619-935d-c549a6c5c090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes per class in each set:\n",
      "Train set:\n",
      "Class 0: 9788 nodes\n",
      "Class 1: 11 nodes\n",
      "Validation set:\n",
      "Class 0: 5271 nodes\n",
      "Class 1: 5 nodes\n",
      "Test set:\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training, validation, and test sets\n",
    "\n",
    "seed_value = 0\n",
    "torch.manual_seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    \n",
    "transform = RandomNodeSplit(split=\"train_rest\", num_val=0.35, num_test=0.0, key='y')\n",
    "graph = transform(graph)\n",
    "\n",
    "# Count the number of nodes per class in each set\n",
    "train_class_counts = Counter(graph.y[graph.train_mask].numpy())\n",
    "val_class_counts = Counter(graph.y[graph.val_mask].numpy())\n",
    "test_class_counts = Counter(graph.y[graph.test_mask].numpy())\n",
    "\n",
    "# Print the results\n",
    "print(\"Number of nodes per class in each set:\")\n",
    "print(\"Train set:\")\n",
    "for class_label, count in train_class_counts.items():\n",
    "    print(f\"Class {class_label}: {count} nodes\")\n",
    "print(\"Validation set:\")\n",
    "for class_label, count in val_class_counts.items():\n",
    "    print(f\"Class {class_label}: {count} nodes\")\n",
    "print(\"Test set:\")\n",
    "#for class_label, count in test_class_counts.items():\n",
    "#    print(f\"Class {class_label}: {count} nodes\")\n",
    "\n",
    "# Calculate and print the percentage of class 1 vs. class 0 in the test set\n",
    "#total_test_nodes = sum(test_class_counts.values())\n",
    "#class_0_nodes = test_class_counts[0]\n",
    "#class_1_nodes = test_class_counts[1]\n",
    "#class_0_percentage = (class_0_nodes / total_test_nodes) * 100\n",
    "#class_1_percentage = (class_1_nodes / total_test_nodes) * 100\n",
    "#print(f\"Percentage of Class 0 in test set: {class_0_percentage:.2f}%\")\n",
    "#print(f\"Percentage of Class 1 in test set: {class_1_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edf5f572-30db-4b0e-a5b0-00e346d884cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.0001800372, Train Acc: 0.9988774365, Val Loss: 0.0001530051, Val Acc: 0.9990523124, Val AUPRC: 0.0008963528, Val ROC-AUC: 0.4560804401\n",
      "Epoch: 2, Loss: 0.0001521754, Train Acc: 0.9988774365, Val Loss: 0.0001598119, Val Acc: 0.9990523124, Val AUPRC: 0.0009993479, Val ROC-AUC: 0.3899070385\n",
      "Epoch: 3, Loss: 0.0000809194, Train Acc: 0.9988774365, Val Loss: 0.0001679866, Val Acc: 0.9990523124, Val AUPRC: 0.0006941144, Val ROC-AUC: 0.3532157086\n",
      "Epoch: 4, Loss: 0.0000630325, Train Acc: 0.9988774365, Val Loss: 0.0001686777, Val Acc: 0.9990523124, Val AUPRC: 0.0010221109, Val ROC-AUC: 0.4531398217\n",
      "Epoch: 5, Loss: 0.0000459529, Train Acc: 0.9988774365, Val Loss: 0.0001673858, Val Acc: 0.9990523124, Val AUPRC: 0.0008497020, Val ROC-AUC: 0.4124454563\n",
      "Epoch: 6, Loss: 0.0000307868, Train Acc: 0.9988774365, Val Loss: 0.0001643297, Val Acc: 0.9990523124, Val AUPRC: 0.0006398039, Val ROC-AUC: 0.3265035098\n",
      "Epoch: 7, Loss: 0.0000241360, Train Acc: 0.9988774365, Val Loss: 0.0001586303, Val Acc: 0.9990523124, Val AUPRC: 0.0006092892, Val ROC-AUC: 0.2957313603\n",
      "Epoch: 8, Loss: 0.0000207323, Train Acc: 0.9988774365, Val Loss: 0.0001521321, Val Acc: 0.9990523124, Val AUPRC: 0.0005977922, Val ROC-AUC: 0.2768734585\n",
      "Epoch: 9, Loss: 0.0000163164, Train Acc: 0.9988774365, Val Loss: 0.0001447927, Val Acc: 0.9990523124, Val AUPRC: 0.0006013848, Val ROC-AUC: 0.2764940239\n",
      "Epoch: 10, Loss: 0.0000136908, Train Acc: 0.9988774365, Val Loss: 0.0001376133, Val Acc: 0.9990523124, Val AUPRC: 0.0006613098, Val ROC-AUC: 0.3464617720\n",
      "Epoch: 11, Loss: 0.0000117331, Train Acc: 0.9988774365, Val Loss: 0.0001305669, Val Acc: 0.9990523124, Val AUPRC: 0.0007380256, Val ROC-AUC: 0.4016315690\n",
      "Epoch: 12, Loss: 0.0000100491, Train Acc: 0.9988774365, Val Loss: 0.0001215478, Val Acc: 0.9990523124, Val AUPRC: 0.0007596241, Val ROC-AUC: 0.4100170746\n",
      "Epoch: 13, Loss: 0.0000087434, Train Acc: 0.9988774365, Val Loss: 0.0001119296, Val Acc: 0.9990523124, Val AUPRC: 0.0007341432, Val ROC-AUC: 0.3949914627\n",
      "Epoch: 14, Loss: 0.0000073409, Train Acc: 0.9988774365, Val Loss: 0.0001016047, Val Acc: 0.9990523124, Val AUPRC: 0.0007676043, Val ROC-AUC: 0.3944981977\n",
      "Epoch: 15, Loss: 0.0000062723, Train Acc: 0.9988774365, Val Loss: 0.0000914568, Val Acc: 0.9990523124, Val AUPRC: 0.0012437740, Val ROC-AUC: 0.4101309050\n",
      "Epoch: 16, Loss: 0.0000062970, Train Acc: 0.9988774365, Val Loss: 0.0000815256, Val Acc: 0.9990523124, Val AUPRC: 0.0021553816, Val ROC-AUC: 0.4146461772\n",
      "Epoch: 17, Loss: 0.0000057788, Train Acc: 0.9988774365, Val Loss: 0.0000718901, Val Acc: 0.9990523124, Val AUPRC: 0.0059254939, Val ROC-AUC: 0.4409030544\n",
      "Epoch: 18, Loss: 0.0000054367, Train Acc: 0.9988774365, Val Loss: 0.0000624871, Val Acc: 0.9990523124, Val AUPRC: 0.0132582433, Val ROC-AUC: 0.5022196927\n",
      "Epoch: 19, Loss: 0.0000049273, Train Acc: 0.9988774365, Val Loss: 0.0000539381, Val Acc: 0.9990523124, Val AUPRC: 0.0061137092, Val ROC-AUC: 0.5184594954\n",
      "Epoch: 20, Loss: 0.0000049896, Train Acc: 0.9988774365, Val Loss: 0.0000459413, Val Acc: 0.9990523124, Val AUPRC: 0.0094298542, Val ROC-AUC: 0.5457408461\n",
      "Epoch: 21, Loss: 0.0000045259, Train Acc: 0.9988774365, Val Loss: 0.0000390228, Val Acc: 0.9990523124, Val AUPRC: 0.0064642457, Val ROC-AUC: 0.5631948397\n",
      "Epoch: 22, Loss: 0.0000043704, Train Acc: 0.9988774365, Val Loss: 0.0000326926, Val Acc: 0.9990523124, Val AUPRC: 0.0055944064, Val ROC-AUC: 0.5704040979\n",
      "Epoch: 23, Loss: 0.0000043307, Train Acc: 0.9988774365, Val Loss: 0.0000274118, Val Acc: 0.9990523124, Val AUPRC: 0.0034848419, Val ROC-AUC: 0.5809903244\n",
      "Epoch: 24, Loss: 0.0000041266, Train Acc: 0.9988774365, Val Loss: 0.0000228565, Val Acc: 0.9990523124, Val AUPRC: 0.0036163599, Val ROC-AUC: 0.5903623601\n",
      "Epoch: 25, Loss: 0.0000041243, Train Acc: 0.9988774365, Val Loss: 0.0000190328, Val Acc: 0.9990523124, Val AUPRC: 0.0033408487, Val ROC-AUC: 0.5970783533\n",
      "Epoch: 26, Loss: 0.0000039464, Train Acc: 0.9988774365, Val Loss: 0.0000160414, Val Acc: 0.9990523124, Val AUPRC: 0.0029857942, Val ROC-AUC: 0.6050085373\n",
      "Epoch: 27, Loss: 0.0000039065, Train Acc: 0.9988774365, Val Loss: 0.0000135322, Val Acc: 0.9990523124, Val AUPRC: 0.0031502566, Val ROC-AUC: 0.6151773857\n",
      "Epoch: 28, Loss: 0.0000034463, Train Acc: 0.9988774365, Val Loss: 0.0000115229, Val Acc: 0.9990523124, Val AUPRC: 0.0027062379, Val ROC-AUC: 0.6582811611\n",
      "Epoch: 29, Loss: 0.0000037037, Train Acc: 0.9988774365, Val Loss: 0.0000098276, Val Acc: 0.9990523124, Val AUPRC: 0.0028464156, Val ROC-AUC: 0.7104534244\n",
      "Epoch: 30, Loss: 0.0000035372, Train Acc: 0.9988774365, Val Loss: 0.0000084332, Val Acc: 0.9990523124, Val AUPRC: 0.0031910950, Val ROC-AUC: 0.7504458357\n",
      "Epoch: 31, Loss: 0.0000034936, Train Acc: 0.9988774365, Val Loss: 0.0000073211, Val Acc: 0.9990523124, Val AUPRC: 0.0033961731, Val ROC-AUC: 0.7715044584\n",
      "Epoch: 32, Loss: 0.0000037311, Train Acc: 0.9988774365, Val Loss: 0.0000063777, Val Acc: 0.9990523124, Val AUPRC: 0.0034148966, Val ROC-AUC: 0.7885790173\n",
      "Epoch: 33, Loss: 0.0000034048, Train Acc: 0.9988774365, Val Loss: 0.0000056496, Val Acc: 0.9990523124, Val AUPRC: 0.0038769129, Val ROC-AUC: 0.8110036046\n",
      "Epoch: 34, Loss: 0.0000033365, Train Acc: 0.9988774365, Val Loss: 0.0000050436, Val Acc: 0.9990523124, Val AUPRC: 0.0050803589, Val ROC-AUC: 0.8317207361\n",
      "Epoch: 35, Loss: 0.0000038727, Train Acc: 0.9988774365, Val Loss: 0.0000045614, Val Acc: 0.9990523124, Val AUPRC: 0.0061350828, Val ROC-AUC: 0.8454562702\n",
      "Epoch: 36, Loss: 0.0000034322, Train Acc: 0.9988774365, Val Loss: 0.0000041919, Val Acc: 0.9990523124, Val AUPRC: 0.0072955191, Val ROC-AUC: 0.8517169418\n",
      "Epoch: 37, Loss: 0.0000031275, Train Acc: 0.9988774365, Val Loss: 0.0000038858, Val Acc: 0.9990523124, Val AUPRC: 0.0074622673, Val ROC-AUC: 0.8498956555\n",
      "Epoch: 38, Loss: 0.0000034369, Train Acc: 0.9988774365, Val Loss: 0.0000036586, Val Acc: 0.9990523124, Val AUPRC: 0.0073335279, Val ROC-AUC: 0.8450388921\n",
      "Epoch: 39, Loss: 0.0000032227, Train Acc: 0.9988774365, Val Loss: 0.0000034718, Val Acc: 0.9990523124, Val AUPRC: 0.0070551616, Val ROC-AUC: 0.8405236198\n",
      "Epoch: 40, Loss: 0.0000032361, Train Acc: 0.9988774365, Val Loss: 0.0000033265, Val Acc: 0.9990523124, Val AUPRC: 0.0072263886, Val ROC-AUC: 0.8425725669\n",
      "Epoch: 41, Loss: 0.0000035417, Train Acc: 0.9988774365, Val Loss: 0.0000032082, Val Acc: 0.9990523124, Val AUPRC: 0.0073214495, Val ROC-AUC: 0.8447353443\n",
      "Epoch: 42, Loss: 0.0000034914, Train Acc: 0.9988774365, Val Loss: 0.0000030705, Val Acc: 0.9990523124, Val AUPRC: 0.0078005170, Val ROC-AUC: 0.8495541643\n",
      "Epoch: 43, Loss: 0.0000035995, Train Acc: 0.9988774365, Val Loss: 0.0000029304, Val Acc: 0.9990523124, Val AUPRC: 0.0078845043, Val ROC-AUC: 0.8498577120\n",
      "Epoch: 44, Loss: 0.0000031841, Train Acc: 0.9988774365, Val Loss: 0.0000028079, Val Acc: 0.9990523124, Val AUPRC: 0.0078865836, Val ROC-AUC: 0.8489091254\n",
      "Epoch: 45, Loss: 0.0000033418, Train Acc: 0.9988774365, Val Loss: 0.0000027104, Val Acc: 0.9990523124, Val AUPRC: 0.0084609775, Val ROC-AUC: 0.8509201290\n",
      "Epoch: 46, Loss: 0.0000032660, Train Acc: 0.9988774365, Val Loss: 0.0000026201, Val Acc: 0.9990523124, Val AUPRC: 0.0093385043, Val ROC-AUC: 0.8536141150\n",
      "Epoch: 47, Loss: 0.0000036649, Train Acc: 0.9988774365, Val Loss: 0.0000025589, Val Acc: 0.9990523124, Val AUPRC: 0.0100222237, Val ROC-AUC: 0.8542591539\n",
      "Epoch: 48, Loss: 0.0000033482, Train Acc: 0.9988774365, Val Loss: 0.0000025030, Val Acc: 0.9990523124, Val AUPRC: 0.0098906542, Val ROC-AUC: 0.8539935496\n",
      "Epoch: 49, Loss: 0.0000035496, Train Acc: 0.9988774365, Val Loss: 0.0000024538, Val Acc: 0.9990523124, Val AUPRC: 0.0105286193, Val ROC-AUC: 0.8538797192\n",
      "Epoch: 50, Loss: 0.0000031315, Train Acc: 0.9988774365, Val Loss: 0.0000024064, Val Acc: 0.9990523124, Val AUPRC: 0.0103321938, Val ROC-AUC: 0.8533105673\n",
      "Epoch: 51, Loss: 0.0000031985, Train Acc: 0.9988774365, Val Loss: 0.0000023764, Val Acc: 0.9990523124, Val AUPRC: 0.0107255648, Val ROC-AUC: 0.8530829065\n",
      "Epoch: 52, Loss: 0.0000031988, Train Acc: 0.9988774365, Val Loss: 0.0000023527, Val Acc: 0.9990523124, Val AUPRC: 0.0109659155, Val ROC-AUC: 0.8528552457\n",
      "Epoch: 53, Loss: 0.0000030056, Train Acc: 0.9988774365, Val Loss: 0.0000023339, Val Acc: 0.9990523124, Val AUPRC: 0.0110045332, Val ROC-AUC: 0.8527034718\n",
      "Epoch: 54, Loss: 0.0000032472, Train Acc: 0.9988774365, Val Loss: 0.0000023202, Val Acc: 0.9990523124, Val AUPRC: 0.0105782789, Val ROC-AUC: 0.8531967369\n",
      "Epoch: 55, Loss: 0.0000030317, Train Acc: 0.9988774365, Val Loss: 0.0000023146, Val Acc: 0.9990523124, Val AUPRC: 0.0107005623, Val ROC-AUC: 0.8525137545\n",
      "Epoch: 56, Loss: 0.0000030859, Train Acc: 0.9988774365, Val Loss: 0.0000023112, Val Acc: 0.9990523124, Val AUPRC: 0.0109460034, Val ROC-AUC: 0.8522860937\n",
      "Epoch: 57, Loss: 0.0000033277, Train Acc: 0.9988774365, Val Loss: 0.0000023192, Val Acc: 0.9990523124, Val AUPRC: 0.0119182285, Val ROC-AUC: 0.8513375071\n",
      "Epoch: 58, Loss: 0.0000029088, Train Acc: 0.9988774365, Val Loss: 0.0000023200, Val Acc: 0.9990523124, Val AUPRC: 0.0129191400, Val ROC-AUC: 0.8513375071\n",
      "Epoch: 59, Loss: 0.0000032813, Train Acc: 0.9988774365, Val Loss: 0.0000023184, Val Acc: 0.9990523124, Val AUPRC: 0.0128557503, Val ROC-AUC: 0.8508821855\n",
      "Epoch: 60, Loss: 0.0000031178, Train Acc: 0.9988774365, Val Loss: 0.0000023187, Val Acc: 0.9990523124, Val AUPRC: 0.0130345190, Val ROC-AUC: 0.8514133940\n",
      "Epoch: 61, Loss: 0.0000031810, Train Acc: 0.9988774365, Val Loss: 0.0000023196, Val Acc: 0.9990523124, Val AUPRC: 0.0131531269, Val ROC-AUC: 0.8510339594\n",
      "Epoch: 62, Loss: 0.0000030167, Train Acc: 0.9988774365, Val Loss: 0.0000023256, Val Acc: 0.9990523124, Val AUPRC: 0.0134230747, Val ROC-AUC: 0.8511857333\n",
      "Epoch: 63, Loss: 0.0000030609, Train Acc: 0.9988774365, Val Loss: 0.0000023318, Val Acc: 0.9990523124, Val AUPRC: 0.0135680392, Val ROC-AUC: 0.8511098463\n",
      "Epoch: 64, Loss: 0.0000032835, Train Acc: 0.9988774365, Val Loss: 0.0000023439, Val Acc: 0.9990523124, Val AUPRC: 0.0137171890, Val ROC-AUC: 0.8503889205\n",
      "Epoch: 65, Loss: 0.0000030866, Train Acc: 0.9988774365, Val Loss: 0.0000023499, Val Acc: 0.9990523124, Val AUPRC: 0.0137977327, Val ROC-AUC: 0.8502371467\n",
      "Epoch: 66, Loss: 0.0000033745, Train Acc: 0.9988774365, Val Loss: 0.0000023574, Val Acc: 0.9990523124, Val AUPRC: 0.0142475388, Val ROC-AUC: 0.8496300512\n",
      "Epoch: 67, Loss: 0.0000030717, Train Acc: 0.9988774365, Val Loss: 0.0000023682, Val Acc: 0.9990523124, Val AUPRC: 0.0145529101, Val ROC-AUC: 0.8486814646\n",
      "Epoch: 68, Loss: 0.0000033947, Train Acc: 0.9988774365, Val Loss: 0.0000023733, Val Acc: 0.9990523124, Val AUPRC: 0.0144371365, Val ROC-AUC: 0.8481881996\n",
      "Epoch: 69, Loss: 0.0000031387, Train Acc: 0.9988774365, Val Loss: 0.0000023775, Val Acc: 0.9990523124, Val AUPRC: 0.0141346131, Val ROC-AUC: 0.8484917473\n",
      "Epoch: 70, Loss: 0.0000031367, Train Acc: 0.9988774365, Val Loss: 0.0000023770, Val Acc: 0.9990523124, Val AUPRC: 0.0142276041, Val ROC-AUC: 0.8479984823\n",
      "Epoch: 71, Loss: 0.0000031682, Train Acc: 0.9988774365, Val Loss: 0.0000023524, Val Acc: 0.9990523124, Val AUPRC: 0.0145510955, Val ROC-AUC: 0.8483399734\n",
      "Epoch: 72, Loss: 0.0000034301, Train Acc: 0.9988774365, Val Loss: 0.0000023348, Val Acc: 0.9990523124, Val AUPRC: 0.0145499831, Val ROC-AUC: 0.8479984823\n",
      "Epoch: 73, Loss: 0.0000029079, Train Acc: 0.9988774365, Val Loss: 0.0000023289, Val Acc: 0.9990523124, Val AUPRC: 0.0145612072, Val ROC-AUC: 0.8488332385\n",
      "Epoch: 74, Loss: 0.0000030687, Train Acc: 0.9988774365, Val Loss: 0.0000023218, Val Acc: 0.9990523124, Val AUPRC: 0.0145641121, Val ROC-AUC: 0.8490608993\n",
      "Epoch: 75, Loss: 0.0000032390, Train Acc: 0.9988774365, Val Loss: 0.0000023139, Val Acc: 0.9990523124, Val AUPRC: 0.0145645454, Val ROC-AUC: 0.8490988427\n",
      "Epoch: 76, Loss: 0.0000031859, Train Acc: 0.9988774365, Val Loss: 0.0000023186, Val Acc: 0.9990523124, Val AUPRC: 0.0145660356, Val ROC-AUC: 0.8490988427\n",
      "Epoch: 77, Loss: 0.0000028229, Train Acc: 0.9988774365, Val Loss: 0.0000023160, Val Acc: 0.9990523124, Val AUPRC: 0.0145613247, Val ROC-AUC: 0.8486814646\n",
      "Epoch: 78, Loss: 0.0000030021, Train Acc: 0.9988774365, Val Loss: 0.0000023144, Val Acc: 0.9990523124, Val AUPRC: 0.0138767749, Val ROC-AUC: 0.8490988427\n",
      "Epoch: 79, Loss: 0.0000029478, Train Acc: 0.9988774365, Val Loss: 0.0000023029, Val Acc: 0.9990523124, Val AUPRC: 0.0138925167, Val ROC-AUC: 0.8500853728\n",
      "Epoch: 80, Loss: 0.0000027592, Train Acc: 0.9988774365, Val Loss: 0.0000022981, Val Acc: 0.9990523124, Val AUPRC: 0.0138980954, Val ROC-AUC: 0.8505027509\n",
      "Epoch: 81, Loss: 0.0000029678, Train Acc: 0.9988774365, Val Loss: 0.0000022891, Val Acc: 0.9990523124, Val AUPRC: 0.0139098889, Val ROC-AUC: 0.8513754506\n",
      "Epoch: 82, Loss: 0.0000030668, Train Acc: 0.9988774365, Val Loss: 0.0000022960, Val Acc: 0.9990523124, Val AUPRC: 0.0138154870, Val ROC-AUC: 0.8519825460\n",
      "Epoch: 83, Loss: 0.0000030191, Train Acc: 0.9988774365, Val Loss: 0.0000022991, Val Acc: 0.9990523124, Val AUPRC: 0.0138238286, Val ROC-AUC: 0.8529690761\n",
      "Epoch: 84, Loss: 0.0000030384, Train Acc: 0.9988774365, Val Loss: 0.0000023014, Val Acc: 0.9990523124, Val AUPRC: 0.0138291601, Val ROC-AUC: 0.8534623411\n",
      "Epoch: 85, Loss: 0.0000029181, Train Acc: 0.9988774365, Val Loss: 0.0000023117, Val Acc: 0.9990523124, Val AUPRC: 0.0138402114, Val ROC-AUC: 0.8542970973\n",
      "Epoch: 86, Loss: 0.0000030305, Train Acc: 0.9988774365, Val Loss: 0.0000023187, Val Acc: 0.9990523124, Val AUPRC: 0.0138475513, Val ROC-AUC: 0.8548662493\n",
      "Epoch: 87, Loss: 0.0000029036, Train Acc: 0.9988774365, Val Loss: 0.0000023264, Val Acc: 0.9990523124, Val AUPRC: 0.0143211133, Val ROC-AUC: 0.8547903624\n",
      "Epoch: 88, Loss: 0.0000030036, Train Acc: 0.9988774365, Val Loss: 0.0000023372, Val Acc: 0.9990523124, Val AUPRC: 0.0153113470, Val ROC-AUC: 0.8542970973\n",
      "Epoch: 89, Loss: 0.0000031180, Train Acc: 0.9988774365, Val Loss: 0.0000023455, Val Acc: 0.9990523124, Val AUPRC: 0.0153238000, Val ROC-AUC: 0.8552456839\n",
      "Epoch: 90, Loss: 0.0000030533, Train Acc: 0.9988774365, Val Loss: 0.0000023480, Val Acc: 0.9990523124, Val AUPRC: 0.0130811560, Val ROC-AUC: 0.8553595143\n",
      "Epoch: 91, Loss: 0.0000029116, Train Acc: 0.9988774365, Val Loss: 0.0000023729, Val Acc: 0.9990523124, Val AUPRC: 0.0135548686, Val ROC-AUC: 0.8550939101\n",
      "Epoch: 92, Loss: 0.0000031053, Train Acc: 0.9988774365, Val Loss: 0.0000023710, Val Acc: 0.9990523124, Val AUPRC: 0.0135686842, Val ROC-AUC: 0.8552456839\n",
      "Epoch: 93, Loss: 0.0000029642, Train Acc: 0.9988774365, Val Loss: 0.0000023504, Val Acc: 0.9990523124, Val AUPRC: 0.0131673552, Val ROC-AUC: 0.8563081009\n",
      "Epoch: 94, Loss: 0.0000031514, Train Acc: 0.9988774365, Val Loss: 0.0000023118, Val Acc: 0.9990523124, Val AUPRC: 0.0130373454, Val ROC-AUC: 0.8573705179\n",
      "Epoch: 95, Loss: 0.0000031078, Train Acc: 0.9988774365, Val Loss: 0.0000023005, Val Acc: 0.9990523124, Val AUPRC: 0.0129939800, Val ROC-AUC: 0.8591918042\n",
      "Epoch: 96, Loss: 0.0000029184, Train Acc: 0.9988774365, Val Loss: 0.0000022668, Val Acc: 0.9990523124, Val AUPRC: 0.0130049899, Val ROC-AUC: 0.8599886170\n",
      "Epoch: 97, Loss: 0.0000027866, Train Acc: 0.9988774365, Val Loss: 0.0000022330, Val Acc: 0.9990523124, Val AUPRC: 0.0128866595, Val ROC-AUC: 0.8611269209\n",
      "Epoch: 98, Loss: 0.0000029749, Train Acc: 0.9988774365, Val Loss: 0.0000022110, Val Acc: 0.9990523124, Val AUPRC: 0.0129066943, Val ROC-AUC: 0.8624169987\n",
      "Epoch: 99, Loss: 0.0000029998, Train Acc: 0.9988774365, Val Loss: 0.0000021999, Val Acc: 0.9990523124, Val AUPRC: 0.0129828809, Val ROC-AUC: 0.8629482072\n",
      "Epoch: 100, Loss: 0.0000030135, Train Acc: 0.9988774365, Val Loss: 0.0000021971, Val Acc: 0.9990523124, Val AUPRC: 0.0129971032, Val ROC-AUC: 0.8638209068\n",
      "Epoch: 101, Loss: 0.0000028436, Train Acc: 0.9988774365, Val Loss: 0.0000022016, Val Acc: 0.9990523124, Val AUPRC: 0.0130155107, Val ROC-AUC: 0.8650350977\n",
      "Epoch: 102, Loss: 0.0000029404, Train Acc: 0.9988774365, Val Loss: 0.0000022093, Val Acc: 0.9990523124, Val AUPRC: 0.0130210450, Val ROC-AUC: 0.8653007020\n",
      "Epoch: 103, Loss: 0.0000026916, Train Acc: 0.9988774365, Val Loss: 0.0000022130, Val Acc: 0.9990523124, Val AUPRC: 0.0130231425, Val ROC-AUC: 0.8654145323\n",
      "Epoch: 104, Loss: 0.0000028264, Train Acc: 0.9988774365, Val Loss: 0.0000022267, Val Acc: 0.9990523124, Val AUPRC: 0.0130442350, Val ROC-AUC: 0.8664390059\n",
      "Epoch: 105, Loss: 0.0000028505, Train Acc: 0.9988774365, Val Loss: 0.0000022520, Val Acc: 0.9990523124, Val AUPRC: 0.0131716235, Val ROC-AUC: 0.8657939670\n",
      "Epoch: 106, Loss: 0.0000029628, Train Acc: 0.9988774365, Val Loss: 0.0000022739, Val Acc: 0.9990523124, Val AUPRC: 0.0131708349, Val ROC-AUC: 0.8656801366\n",
      "Epoch: 107, Loss: 0.0000030629, Train Acc: 0.9988774365, Val Loss: 0.0000023004, Val Acc: 0.9990523124, Val AUPRC: 0.0134737717, Val ROC-AUC: 0.8655283627\n",
      "Epoch: 108, Loss: 0.0000028798, Train Acc: 0.9988774365, Val Loss: 0.0000023378, Val Acc: 0.9990523124, Val AUPRC: 0.0142036575, Val ROC-AUC: 0.8653386454\n",
      "Epoch: 109, Loss: 0.0000028188, Train Acc: 0.9988774365, Val Loss: 0.0000023651, Val Acc: 0.9990523124, Val AUPRC: 0.0155762856, Val ROC-AUC: 0.8650730412\n",
      "Epoch: 110, Loss: 0.0000030760, Train Acc: 0.9988774365, Val Loss: 0.0000023998, Val Acc: 0.9990523124, Val AUPRC: 0.0158020737, Val ROC-AUC: 0.8658698539\n",
      "Epoch: 111, Loss: 0.0000026823, Train Acc: 0.9988774365, Val Loss: 0.0000023951, Val Acc: 0.9990523124, Val AUPRC: 0.0159037894, Val ROC-AUC: 0.8657560235\n",
      "Epoch: 112, Loss: 0.0000029343, Train Acc: 0.9988774365, Val Loss: 0.0000023770, Val Acc: 0.9990523124, Val AUPRC: 0.0158785529, Val ROC-AUC: 0.8646177196\n",
      "Epoch: 113, Loss: 0.0000030507, Train Acc: 0.9988774365, Val Loss: 0.0000023520, Val Acc: 0.9990523124, Val AUPRC: 0.0158621479, Val ROC-AUC: 0.8642382850\n",
      "Epoch: 114, Loss: 0.0000027907, Train Acc: 0.9988774365, Val Loss: 0.0000023023, Val Acc: 0.9990523124, Val AUPRC: 0.0166275835, Val ROC-AUC: 0.8649592108\n",
      "Epoch: 115, Loss: 0.0000030729, Train Acc: 0.9988774365, Val Loss: 0.0000022674, Val Acc: 0.9990523124, Val AUPRC: 0.0159859109, Val ROC-AUC: 0.8651489281\n",
      "Epoch: 116, Loss: 0.0000030229, Train Acc: 0.9988774365, Val Loss: 0.0000022315, Val Acc: 0.9990523124, Val AUPRC: 0.0176523241, Val ROC-AUC: 0.8654145323\n",
      "Epoch: 117, Loss: 0.0000029233, Train Acc: 0.9988774365, Val Loss: 0.0000022083, Val Acc: 0.9990523124, Val AUPRC: 0.0240995770, Val ROC-AUC: 0.8654524758\n",
      "Epoch: 118, Loss: 0.0000030152, Train Acc: 0.9988774365, Val Loss: 0.0000021999, Val Acc: 0.9990523124, Val AUPRC: 0.0240921823, Val ROC-AUC: 0.8649592108\n",
      "Epoch: 119, Loss: 0.0000027027, Train Acc: 0.9988774365, Val Loss: 0.0000021979, Val Acc: 0.9990523124, Val AUPRC: 0.0243049530, Val ROC-AUC: 0.8648074369\n",
      "Epoch: 120, Loss: 0.0000029339, Train Acc: 0.9988774365, Val Loss: 0.0000022067, Val Acc: 0.9990523124, Val AUPRC: 0.0281206357, Val ROC-AUC: 0.8646556631\n",
      "Epoch: 121, Loss: 0.0000025581, Train Acc: 0.9988774365, Val Loss: 0.0000022217, Val Acc: 0.9990523124, Val AUPRC: 0.0333877992, Val ROC-AUC: 0.8642382850\n",
      "Epoch: 122, Loss: 0.0000027017, Train Acc: 0.9988774365, Val Loss: 0.0000022440, Val Acc: 0.9990523124, Val AUPRC: 0.0250484645, Val ROC-AUC: 0.8637070765\n",
      "Epoch: 123, Loss: 0.0000028753, Train Acc: 0.9988774365, Val Loss: 0.0000022749, Val Acc: 0.9990523124, Val AUPRC: 0.0226698382, Val ROC-AUC: 0.8637450199\n",
      "Epoch: 124, Loss: 0.0000029762, Train Acc: 0.9988774365, Val Loss: 0.0000023126, Val Acc: 0.9990523124, Val AUPRC: 0.0196443881, Val ROC-AUC: 0.8634035287\n",
      "Epoch: 125, Loss: 0.0000029974, Train Acc: 0.9988774365, Val Loss: 0.0000023658, Val Acc: 0.9990523124, Val AUPRC: 0.0185293638, Val ROC-AUC: 0.8627964333\n",
      "Epoch: 126, Loss: 0.0000027007, Train Acc: 0.9988774365, Val Loss: 0.0000023865, Val Acc: 0.9990523124, Val AUPRC: 0.0186829248, Val ROC-AUC: 0.8628343768\n",
      "Epoch: 127, Loss: 0.0000031169, Train Acc: 0.9988774365, Val Loss: 0.0000023989, Val Acc: 0.9990523124, Val AUPRC: 0.0185422073, Val ROC-AUC: 0.8635173591\n",
      "Epoch: 128, Loss: 0.0000026193, Train Acc: 0.9988774365, Val Loss: 0.0000024304, Val Acc: 0.9990523124, Val AUPRC: 0.0187004079, Val ROC-AUC: 0.8633655853\n",
      "Epoch: 129, Loss: 0.0000028619, Train Acc: 0.9988774365, Val Loss: 0.0000024481, Val Acc: 0.9990523124, Val AUPRC: 0.0185561531, Val ROC-AUC: 0.8635932461\n",
      "Epoch: 130, Loss: 0.0000026516, Train Acc: 0.9988774365, Val Loss: 0.0000024385, Val Acc: 0.9990523124, Val AUPRC: 0.0187217964, Val ROC-AUC: 0.8640106242\n",
      "Epoch: 131, Loss: 0.0000030175, Train Acc: 0.9988774365, Val Loss: 0.0000024270, Val Acc: 0.9990523124, Val AUPRC: 0.0187253513, Val ROC-AUC: 0.8640865111\n",
      "Epoch: 132, Loss: 0.0000029965, Train Acc: 0.9988774365, Val Loss: 0.0000023660, Val Acc: 0.9990523124, Val AUPRC: 0.0198598575, Val ROC-AUC: 0.8654904193\n",
      "Epoch: 133, Loss: 0.0000026318, Train Acc: 0.9988774365, Val Loss: 0.0000022961, Val Acc: 0.9990523124, Val AUPRC: 0.0337587567, Val ROC-AUC: 0.8661734016\n",
      "Epoch: 134, Loss: 0.0000025691, Train Acc: 0.9988774365, Val Loss: 0.0000022481, Val Acc: 0.9990523124, Val AUPRC: 0.0336093994, Val ROC-AUC: 0.8667804971\n",
      "Epoch: 135, Loss: 0.0000029781, Train Acc: 0.9988774365, Val Loss: 0.0000022211, Val Acc: 0.9990523124, Val AUPRC: 0.0336230796, Val ROC-AUC: 0.8676531967\n",
      "Epoch: 136, Loss: 0.0000027574, Train Acc: 0.9988774365, Val Loss: 0.0000021965, Val Acc: 0.9990523124, Val AUPRC: 0.0419678226, Val ROC-AUC: 0.8682223487\n",
      "Epoch: 137, Loss: 0.0000029144, Train Acc: 0.9988774365, Val Loss: 0.0000021792, Val Acc: 0.9990523124, Val AUPRC: 0.0419825767, Val ROC-AUC: 0.8690950484\n",
      "Epoch: 138, Loss: 0.0000026657, Train Acc: 0.9988774365, Val Loss: 0.0000021824, Val Acc: 0.9990523124, Val AUPRC: 0.0419703647, Val ROC-AUC: 0.8668563840\n",
      "Epoch: 139, Loss: 0.0000029080, Train Acc: 0.9988774365, Val Loss: 0.0000021916, Val Acc: 0.9990523124, Val AUPRC: 0.0420820358, Val ROC-AUC: 0.8616581294\n",
      "Epoch: 140, Loss: 0.0000025400, Train Acc: 0.9988774365, Val Loss: 0.0000022009, Val Acc: 0.9990523124, Val AUPRC: 0.2087744139, Val ROC-AUC: 0.8625687725\n",
      "Epoch: 141, Loss: 0.0000027627, Train Acc: 0.9988774365, Val Loss: 0.0000022184, Val Acc: 0.9990523124, Val AUPRC: 0.2087934129, Val ROC-AUC: 0.8633276418\n",
      "Epoch: 142, Loss: 0.0000027436, Train Acc: 0.9988774365, Val Loss: 0.0000022420, Val Acc: 0.9990523124, Val AUPRC: 0.2088152023, Val ROC-AUC: 0.8671978752\n",
      "Epoch: 143, Loss: 0.0000027676, Train Acc: 0.9988774365, Val Loss: 0.0000022818, Val Acc: 0.9990523124, Val AUPRC: 0.0420020339, Val ROC-AUC: 0.8692847657\n",
      "Epoch: 144, Loss: 0.0000027540, Train Acc: 0.9988774365, Val Loss: 0.0000023082, Val Acc: 0.9990523124, Val AUPRC: 0.0419877622, Val ROC-AUC: 0.8687156137\n",
      "Epoch: 145, Loss: 0.0000030193, Train Acc: 0.9988774365, Val Loss: 0.0000023515, Val Acc: 0.9990523124, Val AUPRC: 0.0336564621, Val ROC-AUC: 0.8687535572\n",
      "Epoch: 146, Loss: 0.0000029375, Train Acc: 0.9988774365, Val Loss: 0.0000023824, Val Acc: 0.9990523124, Val AUPRC: 0.0229338676, Val ROC-AUC: 0.8680705748\n",
      "Epoch: 147, Loss: 0.0000027529, Train Acc: 0.9988774365, Val Loss: 0.0000023971, Val Acc: 0.9990523124, Val AUPRC: 0.0171376848, Val ROC-AUC: 0.8679567445\n",
      "Epoch: 148, Loss: 0.0000026836, Train Acc: 0.9988774365, Val Loss: 0.0000023912, Val Acc: 0.9990523124, Val AUPRC: 0.0171174636, Val ROC-AUC: 0.8671599317\n",
      "Epoch: 149, Loss: 0.0000026103, Train Acc: 0.9988774365, Val Loss: 0.0000023314, Val Acc: 0.9990523124, Val AUPRC: 0.0187763119, Val ROC-AUC: 0.8670461013\n",
      "Epoch: 150, Loss: 0.0000029791, Train Acc: 0.9988774365, Val Loss: 0.0000022568, Val Acc: 0.9990523124, Val AUPRC: 0.0186219965, Val ROC-AUC: 0.8670461013\n",
      "Epoch: 151, Loss: 0.0000026803, Train Acc: 0.9988774365, Val Loss: 0.0000022060, Val Acc: 0.9990523124, Val AUPRC: 0.0197366199, Val ROC-AUC: 0.8673496490\n",
      "Epoch: 152, Loss: 0.0000025984, Train Acc: 0.9988774365, Val Loss: 0.0000021801, Val Acc: 0.9990523124, Val AUPRC: 0.0229015990, Val ROC-AUC: 0.8669322709\n",
      "Epoch: 153, Loss: 0.0000029754, Train Acc: 0.9988774365, Val Loss: 0.0000021723, Val Acc: 0.9990523124, Val AUPRC: 0.0197337938, Val ROC-AUC: 0.8671599317\n",
      "Epoch: 154, Loss: 0.0000027549, Train Acc: 0.9988774365, Val Loss: 0.0000021646, Val Acc: 0.9990523124, Val AUPRC: 0.0286272883, Val ROC-AUC: 0.8674255360\n",
      "Epoch: 155, Loss: 0.0000028787, Train Acc: 0.9988774365, Val Loss: 0.0000021718, Val Acc: 0.9990523124, Val AUPRC: 0.0339326054, Val ROC-AUC: 0.8671599317\n",
      "Epoch: 156, Loss: 0.0000027340, Train Acc: 0.9988774365, Val Loss: 0.0000021629, Val Acc: 0.9990523124, Val AUPRC: 0.0337869350, Val ROC-AUC: 0.8678429141\n",
      "Epoch: 157, Loss: 0.0000028029, Train Acc: 0.9988774365, Val Loss: 0.0000021854, Val Acc: 0.9990523124, Val AUPRC: 0.0339403153, Val ROC-AUC: 0.8674255360\n",
      "Epoch: 158, Loss: 0.0000028439, Train Acc: 0.9988774365, Val Loss: 0.0000022141, Val Acc: 0.9990523124, Val AUPRC: 0.0339278270, Val ROC-AUC: 0.8668563840\n",
      "Epoch: 159, Loss: 0.0000028732, Train Acc: 0.9988774365, Val Loss: 0.0000022573, Val Acc: 0.9990523124, Val AUPRC: 0.0337668397, Val ROC-AUC: 0.8666287232\n",
      "Epoch: 160, Loss: 0.0000029336, Train Acc: 0.9988774365, Val Loss: 0.0000022876, Val Acc: 0.9990523124, Val AUPRC: 0.0337658346, Val ROC-AUC: 0.8664769493\n",
      "Epoch: 161, Loss: 0.0000028846, Train Acc: 0.9988774365, Val Loss: 0.0000023326, Val Acc: 0.9990523124, Val AUPRC: 0.0336233146, Val ROC-AUC: 0.8668563840\n",
      "Epoch: 162, Loss: 0.0000025899, Train Acc: 0.9988774365, Val Loss: 0.0000023464, Val Acc: 0.9990523124, Val AUPRC: 0.0336443475, Val ROC-AUC: 0.8679567445\n",
      "Epoch: 163, Loss: 0.0000027751, Train Acc: 0.9988774365, Val Loss: 0.0000023475, Val Acc: 0.9990523124, Val AUPRC: 0.0336645782, Val ROC-AUC: 0.8692468222\n",
      "Epoch: 164, Loss: 0.0000027237, Train Acc: 0.9988774365, Val Loss: 0.0000023280, Val Acc: 0.9990523124, Val AUPRC: 0.0338337234, Val ROC-AUC: 0.8703471827\n",
      "Epoch: 165, Loss: 0.0000029660, Train Acc: 0.9988774365, Val Loss: 0.0000022970, Val Acc: 0.9990523124, Val AUPRC: 0.0338303614, Val ROC-AUC: 0.8709542781\n",
      "Epoch: 166, Loss: 0.0000029164, Train Acc: 0.9988774365, Val Loss: 0.0000023030, Val Acc: 0.9990523124, Val AUPRC: 0.0338442051, Val ROC-AUC: 0.8714095997\n",
      "Epoch: 167, Loss: 0.0000026366, Train Acc: 0.9988774365, Val Loss: 0.0000023182, Val Acc: 0.9990523124, Val AUPRC: 0.0288478680, Val ROC-AUC: 0.8717510909\n",
      "Epoch: 168, Loss: 0.0000028155, Train Acc: 0.9988774365, Val Loss: 0.0000023441, Val Acc: 0.9990523124, Val AUPRC: 0.0212102859, Val ROC-AUC: 0.8722822994\n",
      "Epoch: 169, Loss: 0.0000026128, Train Acc: 0.9988774365, Val Loss: 0.0000023584, Val Acc: 0.9990523124, Val AUPRC: 0.0213755387, Val ROC-AUC: 0.8723961298\n",
      "Epoch: 170, Loss: 0.0000029049, Train Acc: 0.9988774365, Val Loss: 0.0000023570, Val Acc: 0.9990523124, Val AUPRC: 0.0233370086, Val ROC-AUC: 0.8727755644\n",
      "Epoch: 171, Loss: 0.0000026506, Train Acc: 0.9988774365, Val Loss: 0.0000023399, Val Acc: 0.9990523124, Val AUPRC: 0.0233517962, Val ROC-AUC: 0.8733067729\n",
      "Epoch: 172, Loss: 0.0000026250, Train Acc: 0.9988774365, Val Loss: 0.0000023324, Val Acc: 0.9990523124, Val AUPRC: 0.0201815146, Val ROC-AUC: 0.8731929425\n",
      "Epoch: 173, Loss: 0.0000025559, Train Acc: 0.9988774365, Val Loss: 0.0000023168, Val Acc: 0.9990523124, Val AUPRC: 0.0190886231, Val ROC-AUC: 0.8735723772\n",
      "Epoch: 174, Loss: 0.0000026523, Train Acc: 0.9988774365, Val Loss: 0.0000023072, Val Acc: 0.9990523124, Val AUPRC: 0.0191114673, Val ROC-AUC: 0.8742933030\n",
      "Epoch: 175, Loss: 0.0000027488, Train Acc: 0.9988774365, Val Loss: 0.0000023141, Val Acc: 0.9990523124, Val AUPRC: 0.0188052963, Val ROC-AUC: 0.8744450768\n",
      "Epoch: 176, Loss: 0.0000026948, Train Acc: 0.9988774365, Val Loss: 0.0000023211, Val Acc: 0.9990523124, Val AUPRC: 0.0230995392, Val ROC-AUC: 0.8749762853\n",
      "Epoch: 177, Loss: 0.0000029283, Train Acc: 0.9988774365, Val Loss: 0.0000023225, Val Acc: 0.9990523124, Val AUPRC: 0.0338259478, Val ROC-AUC: 0.8755833808\n",
      "Epoch: 178, Loss: 0.0000027310, Train Acc: 0.9988774365, Val Loss: 0.0000023018, Val Acc: 0.9990523124, Val AUPRC: 0.0421671257, Val ROC-AUC: 0.8758489850\n",
      "Epoch: 179, Loss: 0.0000026566, Train Acc: 0.9988774365, Val Loss: 0.0000022922, Val Acc: 0.9990523124, Val AUPRC: 0.0421620230, Val ROC-AUC: 0.8760007589\n",
      "Epoch: 180, Loss: 0.0000026202, Train Acc: 0.9988774365, Val Loss: 0.0000022942, Val Acc: 0.9990523124, Val AUPRC: 0.0421664777, Val ROC-AUC: 0.8764181370\n",
      "Epoch: 181, Loss: 0.0000026929, Train Acc: 0.9988774365, Val Loss: 0.0000023011, Val Acc: 0.9990523124, Val AUPRC: 0.0255013284, Val ROC-AUC: 0.8766078543\n",
      "Epoch: 182, Loss: 0.0000026320, Train Acc: 0.9988774365, Val Loss: 0.0000023114, Val Acc: 0.9990523124, Val AUPRC: 0.0232686952, Val ROC-AUC: 0.8762284197\n",
      "Epoch: 183, Loss: 0.0000026929, Train Acc: 0.9988774365, Val Loss: 0.0000023038, Val Acc: 0.9990523124, Val AUPRC: 0.0234205189, Val ROC-AUC: 0.8758869285\n",
      "Epoch: 184, Loss: 0.0000026279, Train Acc: 0.9988774365, Val Loss: 0.0000022886, Val Acc: 0.9990523124, Val AUPRC: 0.0234140176, Val ROC-AUC: 0.8756592677\n",
      "Epoch: 185, Loss: 0.0000028514, Train Acc: 0.9988774365, Val Loss: 0.0000022629, Val Acc: 0.9990523124, Val AUPRC: 0.0257762291, Val ROC-AUC: 0.8747486245\n",
      "Epoch: 186, Loss: 0.0000025477, Train Acc: 0.9988774365, Val Loss: 0.0000022524, Val Acc: 0.9990523124, Val AUPRC: 0.0424397234, Val ROC-AUC: 0.8745589072\n",
      "Epoch: 187, Loss: 0.0000026460, Train Acc: 0.9988774365, Val Loss: 0.0000022459, Val Acc: 0.9990523124, Val AUPRC: 0.0425955654, Val ROC-AUC: 0.8734585468\n",
      "Epoch: 188, Loss: 0.0000029523, Train Acc: 0.9988774365, Val Loss: 0.0000022125, Val Acc: 0.9990523124, Val AUPRC: 0.0342462496, Val ROC-AUC: 0.8719028647\n",
      "Epoch: 189, Loss: 0.0000025612, Train Acc: 0.9988774365, Val Loss: 0.0000022114, Val Acc: 0.9990523124, Val AUPRC: 0.0259232119, Val ROC-AUC: 0.8719787517\n",
      "Epoch: 190, Loss: 0.0000027786, Train Acc: 0.9988774365, Val Loss: 0.0000022115, Val Acc: 0.9990523124, Val AUPRC: 0.0259096870, Val ROC-AUC: 0.8694744830\n",
      "Epoch: 191, Loss: 0.0000027683, Train Acc: 0.9988774365, Val Loss: 0.0000022163, Val Acc: 0.9990523124, Val AUPRC: 0.0234061042, Val ROC-AUC: 0.8738379814\n",
      "Epoch: 192, Loss: 0.0000025529, Train Acc: 0.9988774365, Val Loss: 0.0000022443, Val Acc: 0.9990523124, Val AUPRC: 0.0200838687, Val ROC-AUC: 0.8738000379\n",
      "Epoch: 193, Loss: 0.0000025875, Train Acc: 0.9988774365, Val Loss: 0.0000022579, Val Acc: 0.9990523124, Val AUPRC: 0.0179478638, Val ROC-AUC: 0.8746727376\n",
      "Epoch: 194, Loss: 0.0000028221, Train Acc: 0.9988774365, Val Loss: 0.0000022719, Val Acc: 0.9990523124, Val AUPRC: 0.0179775030, Val ROC-AUC: 0.8759248719\n",
      "Epoch: 195, Loss: 0.0000027585, Train Acc: 0.9988774365, Val Loss: 0.0000022714, Val Acc: 0.9990523124, Val AUPRC: 0.0180012155, Val ROC-AUC: 0.8767596282\n",
      "Epoch: 196, Loss: 0.0000026379, Train Acc: 0.9988774365, Val Loss: 0.0000022756, Val Acc: 0.9990523124, Val AUPRC: 0.0180382389, Val ROC-AUC: 0.8774426105\n",
      "Epoch: 197, Loss: 0.0000026058, Train Acc: 0.9988774365, Val Loss: 0.0000023014, Val Acc: 0.9990523124, Val AUPRC: 0.0200971959, Val ROC-AUC: 0.8777461582\n",
      "Epoch: 198, Loss: 0.0000025476, Train Acc: 0.9988774365, Val Loss: 0.0000023031, Val Acc: 0.9990523124, Val AUPRC: 0.0190265265, Val ROC-AUC: 0.8786188579\n",
      "Epoch: 199, Loss: 0.0000028167, Train Acc: 0.9988774365, Val Loss: 0.0000023054, Val Acc: 0.9990523124, Val AUPRC: 0.0234569062, Val ROC-AUC: 0.8788465187\n",
      "Epoch: 200, Loss: 0.0000029408, Train Acc: 0.9988774365, Val Loss: 0.0000023042, Val Acc: 0.9990523124, Val AUPRC: 0.0233317976, Val ROC-AUC: 0.8790362360\n",
      "Epoch: 201, Loss: 0.0000025959, Train Acc: 0.9988774365, Val Loss: 0.0000022977, Val Acc: 0.9990523124, Val AUPRC: 0.0233677061, Val ROC-AUC: 0.8794536141\n",
      "Epoch: 202, Loss: 0.0000027534, Train Acc: 0.9988774365, Val Loss: 0.0000022657, Val Acc: 0.9990523124, Val AUPRC: 0.0233884537, Val ROC-AUC: 0.8800986530\n",
      "Epoch: 203, Loss: 0.0000026085, Train Acc: 0.9988774365, Val Loss: 0.0000022347, Val Acc: 0.9990523124, Val AUPRC: 0.0234000643, Val ROC-AUC: 0.8807436919\n",
      "Epoch: 204, Loss: 0.0000025333, Train Acc: 0.9988774365, Val Loss: 0.0000021747, Val Acc: 0.9990523124, Val AUPRC: 0.0231279177, Val ROC-AUC: 0.8814646177\n",
      "Epoch: 205, Loss: 0.0000029021, Train Acc: 0.9988774365, Val Loss: 0.0000021490, Val Acc: 0.9990523124, Val AUPRC: 0.0232075660, Val ROC-AUC: 0.8825649782\n",
      "Epoch: 206, Loss: 0.0000029416, Train Acc: 0.9988774365, Val Loss: 0.0000021232, Val Acc: 0.9990523124, Val AUPRC: 0.0233094344, Val ROC-AUC: 0.8840068298\n",
      "Epoch: 207, Loss: 0.0000025968, Train Acc: 0.9988774365, Val Loss: 0.0000021037, Val Acc: 0.9990523124, Val AUPRC: 0.0200258621, Val ROC-AUC: 0.8846518687\n",
      "Epoch: 208, Loss: 0.0000029659, Train Acc: 0.9988774365, Val Loss: 0.0000020731, Val Acc: 0.9990523124, Val AUPRC: 0.0197421951, Val ROC-AUC: 0.8861316638\n",
      "Epoch: 209, Loss: 0.0000024581, Train Acc: 0.9988774365, Val Loss: 0.0000020337, Val Acc: 0.9990523124, Val AUPRC: 0.0230679441, Val ROC-AUC: 0.8873458547\n",
      "Epoch: 210, Loss: 0.0000028587, Train Acc: 0.9988774365, Val Loss: 0.0000020079, Val Acc: 0.9990523124, Val AUPRC: 0.0186966275, Val ROC-AUC: 0.8883703282\n",
      "Epoch: 211, Loss: 0.0000025916, Train Acc: 0.9988774365, Val Loss: 0.0000019918, Val Acc: 0.9990523124, Val AUPRC: 0.0187088156, Val ROC-AUC: 0.8881806109\n",
      "Epoch: 212, Loss: 0.0000027706, Train Acc: 0.9988774365, Val Loss: 0.0000019932, Val Acc: 0.9990523124, Val AUPRC: 0.0187375110, Val ROC-AUC: 0.8879908936\n",
      "Epoch: 213, Loss: 0.0000028344, Train Acc: 0.9988774365, Val Loss: 0.0000020011, Val Acc: 0.9990523124, Val AUPRC: 0.0173609947, Val ROC-AUC: 0.8870802504\n",
      "Epoch: 214, Loss: 0.0000025190, Train Acc: 0.9988774365, Val Loss: 0.0000020140, Val Acc: 0.9990523124, Val AUPRC: 0.0165009936, Val ROC-AUC: 0.8864352115\n",
      "Epoch: 215, Loss: 0.0000025559, Train Acc: 0.9988774365, Val Loss: 0.0000020225, Val Acc: 0.9990523124, Val AUPRC: 0.0165268069, Val ROC-AUC: 0.8857142857\n",
      "Epoch: 216, Loss: 0.0000025799, Train Acc: 0.9988774365, Val Loss: 0.0000020245, Val Acc: 0.9990523124, Val AUPRC: 0.0164202771, Val ROC-AUC: 0.8853348511\n",
      "Epoch: 217, Loss: 0.0000027101, Train Acc: 0.9988774365, Val Loss: 0.0000020332, Val Acc: 0.9990523124, Val AUPRC: 0.0152256322, Val ROC-AUC: 0.8848415860\n",
      "Epoch: 218, Loss: 0.0000029179, Train Acc: 0.9988774365, Val Loss: 0.0000020534, Val Acc: 0.9990523124, Val AUPRC: 0.0151819469, Val ROC-AUC: 0.8844621514\n",
      "Epoch: 219, Loss: 0.0000024828, Train Acc: 0.9988774365, Val Loss: 0.0000020686, Val Acc: 0.9990523124, Val AUPRC: 0.0151587185, Val ROC-AUC: 0.8837791690\n",
      "Epoch: 220, Loss: 0.0000026779, Train Acc: 0.9988774365, Val Loss: 0.0000020943, Val Acc: 0.9990523124, Val AUPRC: 0.0151480712, Val ROC-AUC: 0.8836273952\n",
      "Epoch: 221, Loss: 0.0000025446, Train Acc: 0.9988774365, Val Loss: 0.0000021200, Val Acc: 0.9990523124, Val AUPRC: 0.0150920880, Val ROC-AUC: 0.8830961867\n",
      "Epoch: 222, Loss: 0.0000022918, Train Acc: 0.9988774365, Val Loss: 0.0000021167, Val Acc: 0.9990523124, Val AUPRC: 0.0159067834, Val ROC-AUC: 0.8827167520\n",
      "Epoch: 223, Loss: 0.0000028105, Train Acc: 0.9988774365, Val Loss: 0.0000021116, Val Acc: 0.9990523124, Val AUPRC: 0.0157882023, Val ROC-AUC: 0.8828685259\n",
      "Epoch: 224, Loss: 0.0000025695, Train Acc: 0.9988774365, Val Loss: 0.0000020996, Val Acc: 0.9990523124, Val AUPRC: 0.0156472266, Val ROC-AUC: 0.8830202998\n",
      "Epoch: 225, Loss: 0.0000025684, Train Acc: 0.9988774365, Val Loss: 0.0000020936, Val Acc: 0.9990523124, Val AUPRC: 0.0139899799, Val ROC-AUC: 0.8827167520\n",
      "Epoch: 226, Loss: 0.0000025261, Train Acc: 0.9988774365, Val Loss: 0.0000020925, Val Acc: 0.9990523124, Val AUPRC: 0.0140358258, Val ROC-AUC: 0.8831720736\n",
      "Epoch: 227, Loss: 0.0000029106, Train Acc: 0.9988774365, Val Loss: 0.0000020882, Val Acc: 0.9990523124, Val AUPRC: 0.0140682765, Val ROC-AUC: 0.8834376779\n",
      "Epoch: 228, Loss: 0.0000027672, Train Acc: 0.9988774365, Val Loss: 0.0000020955, Val Acc: 0.9990523124, Val AUPRC: 0.0141764586, Val ROC-AUC: 0.8829823563\n",
      "Epoch: 229, Loss: 0.0000027700, Train Acc: 0.9988774365, Val Loss: 0.0000021073, Val Acc: 0.9990523124, Val AUPRC: 0.0144087806, Val ROC-AUC: 0.8828685259\n",
      "Epoch: 230, Loss: 0.0000028844, Train Acc: 0.9988774365, Val Loss: 0.0000021291, Val Acc: 0.9990523124, Val AUPRC: 0.0164537594, Val ROC-AUC: 0.8834376779\n",
      "Epoch: 231, Loss: 0.0000028888, Train Acc: 0.9988774365, Val Loss: 0.0000021526, Val Acc: 0.9990523124, Val AUPRC: 0.0147857771, Val ROC-AUC: 0.8827926390\n",
      "Epoch: 232, Loss: 0.0000026781, Train Acc: 0.9988774365, Val Loss: 0.0000021631, Val Acc: 0.9990523124, Val AUPRC: 0.0149607891, Val ROC-AUC: 0.8823752609\n",
      "Epoch: 233, Loss: 0.0000028653, Train Acc: 0.9988774365, Val Loss: 0.0000021613, Val Acc: 0.9990523124, Val AUPRC: 0.0168120128, Val ROC-AUC: 0.8827546955\n",
      "Epoch: 234, Loss: 0.0000026695, Train Acc: 0.9988774365, Val Loss: 0.0000021334, Val Acc: 0.9990523124, Val AUPRC: 0.0169207750, Val ROC-AUC: 0.8835515083\n",
      "Epoch: 235, Loss: 0.0000024879, Train Acc: 0.9988774365, Val Loss: 0.0000021105, Val Acc: 0.9990523124, Val AUPRC: 0.0167725861, Val ROC-AUC: 0.8829444128\n",
      "Epoch: 236, Loss: 0.0000026143, Train Acc: 0.9988774365, Val Loss: 0.0000020644, Val Acc: 0.9990523124, Val AUPRC: 0.0166771495, Val ROC-AUC: 0.8831341301\n",
      "Epoch: 237, Loss: 0.0000027602, Train Acc: 0.9988774365, Val Loss: 0.0000020341, Val Acc: 0.9990523124, Val AUPRC: 0.0166268122, Val ROC-AUC: 0.8833997344\n",
      "Epoch: 238, Loss: 0.0000025492, Train Acc: 0.9988774365, Val Loss: 0.0000020190, Val Acc: 0.9990523124, Val AUPRC: 0.0167409208, Val ROC-AUC: 0.8833617909\n",
      "Epoch: 239, Loss: 0.0000028593, Train Acc: 0.9988774365, Val Loss: 0.0000020067, Val Acc: 0.9990523124, Val AUPRC: 0.0189813949, Val ROC-AUC: 0.8840827168\n",
      "Epoch: 240, Loss: 0.0000026093, Train Acc: 0.9988774365, Val Loss: 0.0000019965, Val Acc: 0.9990523124, Val AUPRC: 0.0189501474, Val ROC-AUC: 0.8839309429\n",
      "Early stopping!\n",
      "Max AUPRC: 0.2088152023\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.nn.functional import binary_cross_entropy, dropout, leaky_relu\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_auc_score  \n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# suppose y is your target vector\n",
    "y = graph.y.cpu().numpy()\n",
    "\n",
    "# Count number of occurrences of each class\n",
    "class_counts = np.bincount(y)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = 1. / class_counts\n",
    "\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "\n",
    "\n",
    "class GraphSAGEModel(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_layers, num_classes, dropout_rate=0.5):\n",
    "        super(GraphSAGEModel, self).__init__()\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.layers.append(SAGEConv(num_node_features, hidden_layers[0]))\n",
    "        for i in range(1, len(hidden_layers)):\n",
    "            self.layers.append(SAGEConv(hidden_layers[i - 1], hidden_layers[i]))\n",
    "        self.layers.append(SAGEConv(hidden_layers[-1], num_classes))\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.bn_layers = torch.nn.ModuleList([torch.nn.BatchNorm1d(size) for size in hidden_layers])\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        for i, conv in enumerate(self.layers[:-1]):\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.bn_layers[i](x)\n",
    "            x = torch.nn.functional.leaky_relu(x)\n",
    "            x = torch.nn.functional.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "        x = self.layers[-1](x, edge_index)\n",
    "        return torch.sigmoid(x.view(-1))\n",
    "    \n",
    "    \n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets.long()].view(-1, 1)\n",
    "            logpt = -binary_cross_entropy(inputs, targets, reduction='none')\n",
    "            logpt = logpt * alpha_t\n",
    "        else:\n",
    "            logpt = -binary_cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(logpt)\n",
    "        F_loss = -((1 - pt) ** self.gamma) * logpt\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "\n",
    "        \n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=100, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def step(self, val_loss):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\\\n",
    "            \n",
    "            \n",
    "def train(model, data, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)[data.train_mask].squeeze()\n",
    "    loss = loss_fn(out, data.y[data.train_mask].float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def evaluate(model, data, mask, loss_fn):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)[mask].squeeze()\n",
    "        preds = (out > 0.90).long()  \n",
    "        loss = loss_fn(out, data.y[mask].float())\n",
    "        accuracy = preds.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        precision, recall, _ = precision_recall_curve(data.y[mask].cpu(), out.cpu())\n",
    "        auprc = auc(recall, precision)\n",
    "        roc_auc = roc_auc_score(data.y[mask].cpu(), out.cpu())  # Compute ROC-AUC\n",
    "    return loss.item(), accuracy, auprc, roc_auc  # Return ROC-AUC along with other metrics\n",
    "\n",
    "\n",
    "def main():\n",
    "    # initialization\n",
    "    hidden_layers = [512, 512, 512, 512, 512, 512, 512, 512] \n",
    "    model = GraphSAGEModel(graph.num_node_features, hidden_layers, 1, dropout_rate=0.1).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=100)\n",
    "    loss_fn = FocalLoss(alpha=class_weights, gamma=2)\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=100, min_delta=0.0001)\n",
    "\n",
    "    # Move your graph data to device\n",
    "    graph.x = graph.x.to(device)\n",
    "    graph.edge_index = graph.edge_index.to(device)\n",
    "    graph.y = graph.y.to(device)\n",
    "    graph.train_mask = graph.train_mask.to(device)\n",
    "    graph.val_mask = graph.val_mask.to(device)\n",
    "    graph.test_mask = graph.test_mask.to(device)\n",
    "\n",
    "    max_auprc = 0\n",
    "    \n",
    "    for epoch in range(1000):\n",
    "        train_loss = train(model, graph, loss_fn, optimizer)\n",
    "        _, train_acc, _, _ = evaluate(model, graph, graph.train_mask, loss_fn)  # Include placeholders for the extra values\n",
    "        val_loss, val_acc, val_auprc, val_roc_auc = evaluate(model, graph, graph.val_mask, loss_fn)  # Unpack ROC-AUC\n",
    "        print(f'Epoch: {epoch+1}, Loss: {train_loss:.10f}, Train Acc: {train_acc:.10f}, Val Loss: {val_loss:.10f}, Val Acc: {val_acc:.10f}, Val AUPRC: {val_auprc:.10f}, Val ROC-AUC: {val_roc_auc:.10f}')  \n",
    "        max_auprc = max(max_auprc, val_auprc)\n",
    "        \n",
    "        # Reduce learning rate when validation loss plateaus\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping.step(-val_auprc)  # Pass -val_auprc because we want to maximize it\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping!\")\n",
    "            print(f'Max AUPRC: {max_auprc:.10f}')\n",
    "            \n",
    "            # Final evaluation on the test set\n",
    "            #test_loss, test_acc, test_auprc, test_roc_auc = evaluate(model, graph, graph.test_mask, loss_fn)\n",
    "            #print(f'Test Loss: {test_loss:.10f}, Test Acc: {test_acc:.10f}, Test AUPRC: {test_auprc:.10f}, Test ROC-AUC: {test_roc_auc:.10f}')\n",
    "            break\n",
    "        \n",
    "\n",
    "# Call the main function\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c9b657d-24ad-40d1-8a0c-2d7c8b5a5e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220.34180146696"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.2088152023 / (5 / 5276)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eba621-d105-486a-9b89-48b84a25b614",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
